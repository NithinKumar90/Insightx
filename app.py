import streamlit as st
import pandas as pd
import json
import re
from openai import OpenAI

# â”€â”€ Page config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="InsightX â€“ Payment Analytics",
    page_icon="ðŸ’³",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€ Custom CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@400;600;700&display=swap');

/* â”€â”€ CSS Variables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
:root {
    --gold:    #f0b429;
    --amber:   #f59e0b;
    --coral:   #ff6b6b;
    --teal:    #00d4aa;
    --ice:     #a8edea;
    --deep:    #060810;
    --surface: #0c0e1a;
    --panel:   #111420;
    --border:  #1c1f33;
    --text:    #eef0f8;
    --muted:   #5a5f7a;
}

/* â”€â”€ Keyframes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
@keyframes fadeUp {
    from { opacity:0; transform:translateY(20px) scale(0.98); }
    to   { opacity:1; transform:translateY(0)    scale(1); }
}
@keyframes glowPulse {
    0%,100% { box-shadow: 0 0 12px rgba(240,180,41,0.15), 0 0 40px rgba(240,180,41,0.05); }
    50%     { box-shadow: 0 0 24px rgba(240,180,41,0.35), 0 0 80px rgba(240,180,41,0.12); }
}
@keyframes shimmerGold {
    0%   { background-position: -600px 0; }
    100% { background-position:  600px 0; }
}
@keyframes borderRotate {
    0%   { background-position: 0% 50%; }
    50%  { background-position: 100% 50%; }
    100% { background-position: 0% 50%; }
}
@keyframes floatY {
    0%,100% { transform: translateY(0px) rotate(0deg); }
    33%     { transform: translateY(-8px) rotate(1deg); }
    66%     { transform: translateY(-4px) rotate(-1deg); }
}
@keyframes scanDown {
    0%   { transform:translateY(-100%); opacity:0; }
    10%  { opacity:0.06; }
    90%  { opacity:0.06; }
    100% { transform:translateY(100vh); opacity:0; }
}
@keyframes popIn {
    0%   { opacity:0; transform:scale(0.8) translateY(10px); }
    70%  { transform:scale(1.03) translateY(-2px); }
    100% { opacity:1; transform:scale(1) translateY(0); }
}
@keyframes typewriter {
    from { width:0; }
    to   { width:100%; }
}
@keyframes gradientShift {
    0%,100% { background-position:0% 50%; }
    50%     { background-position:100% 50%; }
}
@keyframes orbitGlow {
    0%   { transform:rotate(0deg) translateX(30px) rotate(0deg);   opacity:0.6; }
    100% { transform:rotate(360deg) translateX(30px) rotate(-360deg); opacity:0.6; }
}

/* â”€â”€ Base â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
html, body, [class*="css"] {
    font-family: 'Outfit', sans-serif;
    background-color: var(--deep);
    color: var(--text);
}
h1,h2,h3 { font-family: 'Outfit', sans-serif; font-weight:800; }

.stApp {
    background:
        radial-gradient(ellipse 80% 60% at 10% 10%, rgba(240,180,41,0.04) 0%, transparent 60%),
        radial-gradient(ellipse 60% 80% at 90% 90%, rgba(0,212,170,0.04) 0%, transparent 60%),
        radial-gradient(ellipse 40% 40% at 50% 50%, rgba(255,107,107,0.02) 0%, transparent 70%),
        linear-gradient(160deg, #060810 0%, #0a0c18 50%, #060810 100%);
    min-height:100vh;
}

/* Animated scan line */
.stApp::before {
    content:'';
    position:fixed; top:0; left:0;
    width:100%; height:150px;
    background:linear-gradient(transparent, rgba(240,180,41,0.04), transparent);
    animation:scanDown 10s linear infinite;
    pointer-events:none; z-index:0;
}

/* â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
section[data-testid="stSidebar"] {
    background:linear-gradient(180deg, #080a16 0%, #0c0e1c 100%) !important;
    border-right:1px solid var(--border);
    box-shadow:6px 0 40px rgba(0,0,0,0.5), 2px 0 0 rgba(240,180,41,0.08);
}

/* â”€â”€ Header Title â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.header-title {
    font-family:'Outfit',sans-serif;
    font-size:2.2rem;
    font-weight:900;
    background:linear-gradient(90deg, #f0b429, #ff6b6b, #00d4aa, #f0b429);
    background-size:300% 100%;
    -webkit-background-clip:text;
    -webkit-text-fill-color:transparent;
    background-clip:text;
    animation:gradientShift 4s ease infinite;
    letter-spacing:-0.02em;
}
.header-sub {
    color:var(--muted);
    font-size:0.88rem;
    margin-top:2px;
    letter-spacing:0.04em;
    text-transform:uppercase;
}

/* â”€â”€ Chat Messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.user-msg {
    background:linear-gradient(135deg, #131627 0%, #0f1220 100%);
    border-left:3px solid var(--gold);
    border-radius:0 16px 16px 0;
    padding:16px 20px;
    margin:12px 0;
    font-size:0.95rem;
    animation:fadeUp 0.4s cubic-bezier(0.16,1,0.3,1) both;
    position:relative; overflow:hidden;
}
.user-msg::before {
    content:'';
    position:absolute; top:0; left:-100%;
    width:50%; height:100%;
    background:linear-gradient(90deg, transparent, rgba(240,180,41,0.06), transparent);
    animation:shimmerGold 4s ease-in-out infinite;
}

.assistant-msg {
    background:linear-gradient(135deg, #0d1020 0%, #0f1222 100%);
    border-left:3px solid var(--teal);
    border-radius:0 16px 16px 0;
    padding:16px 20px;
    margin:12px 0;
    font-size:0.95rem;
    line-height:1.85;
    animation:fadeUp 0.5s cubic-bezier(0.16,1,0.3,1) both;
    box-shadow:0 4px 32px rgba(0,212,170,0.06), 0 1px 0 rgba(0,212,170,0.1);
    position:relative;
}

/* â”€â”€ Stat Cards â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.stat-card {
    background:linear-gradient(135deg, #0f1220 0%, #141728 100%);
    border:1px solid var(--border);
    border-radius:14px;
    padding:16px;
    text-align:center;
    margin-bottom:8px;
    transition:all 0.3s cubic-bezier(0.16,1,0.3,1);
    animation:popIn 0.5s cubic-bezier(0.16,1,0.3,1) both;
    cursor:default;
    position:relative;
    overflow:hidden;
}
.stat-card::after {
    content:'';
    position:absolute; inset:0;
    border-radius:14px;
    background:linear-gradient(135deg, rgba(240,180,41,0.04), transparent);
    opacity:0;
    transition:opacity 0.3s ease;
}
.stat-card:hover {
    transform:translateY(-4px) scale(1.02);
    border-color:rgba(240,180,41,0.3);
    box-shadow:0 12px 40px rgba(240,180,41,0.12), 0 4px 16px rgba(0,0,0,0.4);
    animation:glowPulse 2s ease-in-out infinite;
}
.stat-card:hover::after { opacity:1; }

.stat-number {
    font-family:'JetBrains Mono',monospace;
    font-size:1.8rem;
    font-weight:700;
    background:linear-gradient(135deg, var(--gold), var(--amber));
    -webkit-background-clip:text;
    -webkit-text-fill-color:transparent;
    background-clip:text;
    letter-spacing:-0.03em;
}
.stat-label {
    font-size:0.7rem;
    color:var(--muted);
    text-transform:uppercase;
    letter-spacing:0.1em;
    margin-top:4px;
    font-weight:500;
}

/* â”€â”€ Sidebar Buttons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.stButton > button {
    background:linear-gradient(135deg, #0f1220, #141728) !important;
    border:1px solid var(--border) !important;
    color:#8890b0 !important;
    border-radius:10px !important;
    font-size:0.81rem !important;
    font-family:'Outfit',sans-serif !important;
    font-weight:500 !important;
    transition:all 0.25s cubic-bezier(0.16,1,0.3,1) !important;
    letter-spacing:0.01em !important;
}
.stButton > button:hover {
    background:linear-gradient(135deg, #181b2e, #1c2038) !important;
    border-color:rgba(240,180,41,0.4) !important;
    color:var(--gold) !important;
    transform:translateX(5px) !important;
    box-shadow:0 4px 20px rgba(240,180,41,0.15), -3px 0 0 var(--gold) !important;
}

/* â”€â”€ Stop Button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.stop-btn-container {
    display:flex;
    justify-content:center;
    margin:8px 0;
}
.stop-btn {
    background:linear-gradient(135deg, #1a0a0a, #200d0d) !important;
    border:1px solid rgba(255,107,107,0.4) !important;
    color:var(--coral) !important;
    border-radius:20px !important;
    padding:6px 20px !important;
    font-size:0.82rem !important;
    font-family:'Outfit',sans-serif !important;
    font-weight:600 !important;
    cursor:pointer !important;
    transition:all 0.2s ease !important;
    letter-spacing:0.05em !important;
}
.stop-btn:hover {
    background:linear-gradient(135deg, #200d0d, #2a1010) !important;
    border-color:var(--coral) !important;
    box-shadow:0 0 20px rgba(255,107,107,0.3) !important;
    transform:scale(1.04) !important;
}

/* â”€â”€ Chat Input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
div[data-testid="stChatInput"] {
    border-top:1px solid var(--border);
    background:linear-gradient(180deg, transparent, rgba(12,14,26,0.95));
    padding-top:10px;
}
div[data-testid="stChatInput"] textarea {
    background:#0f1220 !important;
    border:1px solid var(--border) !important;
    color:var(--text) !important;
    border-radius:14px !important;
    font-family:'Outfit',sans-serif !important;
    font-size:0.95rem !important;
    transition:border-color 0.25s ease, box-shadow 0.25s ease !important;
}
div[data-testid="stChatInput"] textarea:focus {
    border-color:var(--gold) !important;
    box-shadow:0 0 0 3px rgba(240,180,41,0.12), 0 4px 20px rgba(240,180,41,0.08) !important;
}

/* â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
[data-testid="stMetric"] {
    background:linear-gradient(135deg, #0f1220, #141728);
    border:1px solid var(--border);
    border-radius:12px;
    padding:14px !important;
    animation:popIn 0.5s ease both;
    transition:all 0.25s ease;
}
[data-testid="stMetric"]:hover {
    transform:translateY(-3px);
    border-color:rgba(0,212,170,0.25);
    box-shadow:0 8px 24px rgba(0,212,170,0.08);
}
[data-testid="stMetricValue"] {
    color:var(--teal) !important;
    font-family:'JetBrains Mono',monospace !important;
    font-weight:700 !important;
}

/* â”€â”€ Dataframe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
[data-testid="stDataFrame"] {
    border:1px solid var(--border) !important;
    border-radius:12px !important;
    overflow:hidden;
    animation:fadeUp 0.4s ease both;
}

/* â”€â”€ Expander â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
details {
    border:1px solid var(--border) !important;
    border-radius:12px !important;
    background:#0a0c18 !important;
    margin-top:10px;
    transition:border-color 0.25s ease;
}
details:hover { border-color:rgba(240,180,41,0.2) !important; }
details summary {
    color:var(--muted) !important;
    font-size:0.83rem !important;
    cursor:pointer;
    font-family:'Outfit',sans-serif !important;
    transition:color 0.2s ease;
    letter-spacing:0.02em;
}
details summary:hover { color:var(--gold) !important; }

/* â”€â”€ Scrollbar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
::-webkit-scrollbar { width:5px; }
::-webkit-scrollbar-track { background:var(--deep); }
::-webkit-scrollbar-thumb {
    background:linear-gradient(180deg, var(--gold), var(--coral), var(--teal));
    border-radius:4px;
}

/* â”€â”€ Tabs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.stTabs [data-baseweb="tab"] {
    color:var(--muted) !important;
    font-family:'Outfit',sans-serif !important;
    font-size:0.85rem !important;
    font-weight:500 !important;
    transition:color 0.2s ease !important;
}
.stTabs [aria-selected="true"] {
    color:var(--gold) !important;
    border-bottom-color:var(--gold) !important;
}

/* â”€â”€ Dividers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
hr { border-color:var(--border) !important; opacity:0.6; }

/* â”€â”€ Success/Error â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.stSuccess {
    background:rgba(0,212,170,0.07) !important;
    border:1px solid rgba(0,212,170,0.2) !important;
    border-radius:10px !important;
}
.stError {
    background:rgba(255,107,107,0.07) !important;
    border:1px solid rgba(255,107,107,0.2) !important;
    border-radius:10px !important;
}
.stInfo {
    background:rgba(240,180,41,0.06) !important;
    border:1px solid rgba(240,180,41,0.18) !important;
    border-radius:10px !important;
}

/* â”€â”€ Spinner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.stSpinner > div { border-top-color:var(--gold) !important; }

/* â”€â”€ Float animation class â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.float-icon { display:inline-block; animation:floatY 4s ease-in-out infinite; }

/* â”€â”€ Badge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.badge {
    display:inline-block;
    background:linear-gradient(135deg, rgba(240,180,41,0.12), rgba(240,180,41,0.06));
    border:1px solid rgba(240,180,41,0.25);
    color:var(--gold);
    border-radius:20px;
    padding:3px 12px;
    font-size:0.74rem;
    font-weight:600;
    letter-spacing:0.06em;
    text-transform:uppercase;
}
</style>
""", unsafe_allow_html=True)

# â”€â”€ Chat History Persistence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import json
from datetime import datetime

HISTORY_DIR = "chat_histories"
os.makedirs(HISTORY_DIR, exist_ok=True)

def save_chat(session_name, messages):
    """Save chat session to JSON file."""
    path = os.path.join(HISTORY_DIR, f"{session_name}.json")
    with open(path, "w") as f:
        json.dump({
            "session_name": session_name,
            "saved_at": datetime.now().strftime("%d %b %Y, %I:%M %p"),
            "messages": messages
        }, f, indent=2)

def load_chat(session_name):
    """Load chat session from JSON file."""
    path = os.path.join(HISTORY_DIR, f"{session_name}.json")
    if os.path.exists(path):
        with open(path) as f:
            return json.load(f)
    return None

def get_all_sessions():
    """Get all saved chat sessions sorted by newest first."""
    sessions = []
    for fname in os.listdir(HISTORY_DIR):
        if fname.endswith(".json"):
            path = os.path.join(HISTORY_DIR, fname)
            try:
                with open(path) as f:
                    data = json.load(f)
                sessions.append({
                    "file": fname,
                    "name": data.get("session_name", fname),
                    "saved_at": data.get("saved_at", "Unknown"),
                    "msg_count": len(data.get("messages", [])),
                    "preview": data["messages"][0]["content"][:50] + "..." if data.get("messages") else "Empty"
                })
            except Exception:
                pass
    return sorted(sessions, key=lambda x: x["file"], reverse=True)

def delete_session(session_name):
    """Delete a saved session."""
    path = os.path.join(HISTORY_DIR, f"{session_name}.json")
    if os.path.exists(path):
        os.remove(path)

# â”€â”€ LM Studio Client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def get_client():
    return OpenAI(base_url="http://127.0.0.1:1234/v1", api_key="lm-studio")

# â”€â”€ Load & Normalise Columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_data():
    df = pd.read_csv("upi_transactions_2024.csv")

    # Normalise: strip, lowercase, spacesâ†’underscore, remove brackets
    df.columns = (
        df.columns
        .str.strip()
        .str.lower()
        .str.replace(" ", "_", regex=False)
        .str.replace(r"[()]", "", regex=True)
    )

    # "amount_inr" may come out as "amount_inr" or "amount_inr_" etc.
    # Specifically handle "amount (INR)" â†’ becomes "amount_inr" after normalisation âœ“
    # But just in case, rename any column containing "amount" to "amount_inr"
    for col in df.columns:
        if "amount" in col and col != "amount_inr":
            df.rename(columns={col: "amount_inr"}, inplace=True)
            break

    # Parse timestamp
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")

    return df

# â”€â”€ Data Summary for LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_data_summary(df):
    def safe(fn):
        try: return fn()
        except: return "N/A"

    summary = {
        "total_transactions": len(df),
        "columns": list(df.columns),
        "date_range": safe(lambda: f"{df['timestamp'].min()} to {df['timestamp'].max()}"),
        "transaction_types": safe(lambda: df["transaction_type"].value_counts().to_dict()),
        "overall_success_rate": safe(lambda: f"{(df['transaction_status'] == 'SUCCESS').mean()*100:.1f}%"),
        "overall_failure_rate": safe(lambda: f"{(df['transaction_status'] == 'FAILED').mean()*100:.1f}%"),
        "avg_amount_inr": safe(lambda: f"â‚¹{df['amount_inr'].mean():.2f}"),
        "fraud_flag_rate": safe(lambda: f"{df['fraud_flag'].mean()*100:.2f}%"),
        "merchant_categories": safe(lambda: df["merchant_category"].dropna().unique().tolist()),
        "sender_states": safe(lambda: df["sender_state"].dropna().unique().tolist()),
        "banks": safe(lambda: df["sender_bank"].dropna().unique().tolist()),
        "age_groups": safe(lambda: df["sender_age_group"].dropna().unique().tolist()),
        "device_types": safe(lambda: df["device_type"].dropna().unique().tolist()),
        "network_types": safe(lambda: df["network_type"].dropna().unique().tolist()),
    }
    return json.dumps(summary, default=str)


# â”€â”€ Smart Query Router â€” handles complex queries directly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def route_query(user_query, df):
    q = user_query.lower()

    def code(*lines):
        return "\n".join(lines)

    # â”€â”€ Explainability & Strategic questions â†’ skip router, let LLM answer â”€â”€
    # These need explanation not data computation
    # â”€â”€ Explain questions: run real pandas first, then LLM explains â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Q26: "how did you determine which state" â†’ get real state data
    if "how did you determine" in q and "state" in q:
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('sender_state').size()",
            "total  = df.groupby('sender_state').size()",
            "rate   = (failed / total * 100).round(2)",
            "result = pd.DataFrame({'State': rate.index.tolist(), 'Failed': failed.values.tolist(), 'Total': total.values.tolist(), 'Failure_Rate_%': rate.values.tolist()}).sort_values('Failure_Rate_%', ascending=False).reset_index(drop=True)"
        )

    # Q27: "why does education have the highest" â†’ get real merchant data
    if ("why does education" in q or ("why" in q and "education" in q and "average" in q)):
        return code(
            "g = df[df['merchant_category'].notna()].groupby('merchant_category')['amount_inr'].mean().round(2).reset_index()",
            "g.columns = ['Category', 'Avg_Amount_INR']",
            "result = g.sort_values('Avg_Amount_INR', ascending=False).reset_index(drop=True)"
        )

    # Q29: "explain how weekend vs weekday" â†’ get real weekend data
    if ("explain how weekend" in q or ("explain" in q and "weekend" in q and "weekday" in q)):
        return code(
            "wk0 = df[df['is_weekend']==0]",
            "wk1 = df[df['is_weekend']==1]",
            "result = pd.DataFrame({'Period': ['Weekday','Weekend'], 'Transactions': [len(wk0),len(wk1)], 'Failed': [int((wk0['transaction_status']=='FAILED').sum()),int((wk1['transaction_status']=='FAILED').sum())], 'Failure_Rate_%': [round((wk0['transaction_status']=='FAILED').mean()*100,2),round((wk1['transaction_status']=='FAILED').mean()*100,2)]})"
        )

    explain_phrases = [
        "how did you calculate", "how did you determine", "walk me through",
        "explain how", "explain why", "explain the difference",
        "how confident", "limitations of this data", "in simple terms",
        "step by step", "why does education", "why does 3g",
        "how would you build", "how would you detect", "how would this",
        "design a smart", "build a risk score", "which 3 metrics",
        "what machine learning", "production-ready", "scale to 10",
        "if failure rate crosses", "fraud patterns are shifting",
        "what additional data", "what would a production",
        "ceo dashboard",
        "how is the average", "what does it tell us",
        "what does a 0", "actually mean in real numbers", "mean in real",
        "what does the", "what do these numbers",
    ]
    if any(phrase in q for phrase in explain_phrases):
        return None  # Skip router â€” LLM will explain directly

    # â”€â”€ Basic overall stats â€” must be FIRST to avoid wrong routing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # Overall success rate
    if ("success rate" in q or "successful" in q) and not any(x in q.split() for x in ["bank","state","age","device","network","type","merchant","compare"]):
        return code(
            "total = len(df)",
            "success = int((df['transaction_status']=='SUCCESS').sum())",
            "failed  = int((df['transaction_status']=='FAILED').sum())",
            "success_rate = round(success/total*100, 2)",
            "failure_rate = round(failed/total*100, 2)",
            "result = pd.DataFrame({'Metric': ['Success Rate','Failure Rate','Total Transactions'], 'Count': [success, failed, total], 'Percentage': [success_rate, failure_rate, 100.0]})"
        )

    # Overall failure rate
    if ("failure rate" in q or "failed rate" in q or "fail rate" in q) and not any(x in q.split() for x in ["bank","state","age","device","network","type","merchant","compare","recharge","p2p","p2m","bill"]):
        return code(
            "total = len(df)",
            "failed  = int((df['transaction_status']=='FAILED').sum())",
            "success = int((df['transaction_status']=='SUCCESS').sum())",
            "failure_rate = round(failed/total*100, 2)",
            "success_rate = round(success/total*100, 2)",
            "result = pd.DataFrame({'Metric': ['Failure Rate','Success Rate','Total Transactions'], 'Count': [failed, success, total], 'Percentage': [failure_rate, success_rate, 100.0]})"
        )

    # Overall fraud flag rate
    if ("fraud" in q or "flag" in q) and ("overall" in q or "total" in q or "what is" in q or "whats" in q or "rate" in q) and not any(x in q.split() for x in ["bank","state","age","device","network","type","merchant","compare","profile","risk"]):
        return code(
            "total   = len(df)",
            "flagged = int(df['fraud_flag'].sum())",
            "rate    = round(df['fraud_flag'].mean()*100, 4)",
            "result  = pd.DataFrame({'Metric': ['Fraud Flag Rate','Total Flagged','Total Transactions'], 'Value': [str(rate)+'%', flagged, total]})"
        )

    # â”€â”€ Merchant category avg amount â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("merchant" in q or "categor" in q) and ("average" in q or "avg" in q or "highest" in q or "most" in q or "high" in q) and ("amount" in q or "spend" in q or "value" in q):
        return code(
            "tmp = df[df['merchant_category'].notna()].groupby('merchant_category')['amount_inr'].mean().round(2).sort_values(ascending=False).reset_index()",
            "tmp.columns = ['Merchant_Category','Avg_Amount_INR']",
            "result = tmp"
        )

    # Average transaction amount
    if ("average" in q or "avg" in q or "mean" in q) and ("amount" in q or "transaction amount" in q or "value" in q or "transaction value" in q) and not any(x in q.split() for x in ["bank","state","age","device","network","type","merchant","compare"]):
        return code(
            "stats = df['amount_inr'].describe()",
            "result = pd.DataFrame({",
            "    'Metric': ['Average (Mean)', 'Median', 'Min', 'Max', 'Std Dev', 'Total Transactions'],",
            "    'Value': [",
            "        round(df['amount_inr'].mean(), 2),",
            "        round(df['amount_inr'].median(), 2),",
            "        round(df['amount_inr'].min(), 2),",
            "        round(df['amount_inr'].max(), 2),",
            "        round(df['amount_inr'].std(), 2),",
            "        len(df)",
            "    ]",
            "})"
        )

    # â”€â”€ Bank failure rate for iOS â€” MUST be before device check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "ios" in q and ("bank" in q or len(q.split()) <= 5):
        return code(
            "ios = df[df['device_type']=='iOS']",
            "failed = ios[ios['transaction_status']=='FAILED'].groupby('sender_bank').size()",
            "total  = ios.groupby('sender_bank').size()",
            "rate   = (failed / total * 100).round(2)",
            "result = pd.DataFrame({'Bank': rate.index.tolist(), 'Failed': [int(failed[b]) for b in rate.index], 'Total': [int(total[b]) for b in rate.index], 'Failure_Rate_%': rate.values.tolist()}).sort_values('Failure_Rate_%', ascending=False).reset_index(drop=True)"
        )

    # â”€â”€ Device type failure rates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("failure rate" in q or "fail" in q) and ("android" in q or "ios" in q or "device" in q):
        txn = "P2P" if "p2p" in q else ("P2M" if "p2m" in q else None)
        if txn:
            return code(
                f"df_f = df[df['transaction_type']=='{txn}']",
                "failed = df_f[df_f['transaction_status']=='FAILED'].groupby('device_type').size()",
                "total  = df_f.groupby('device_type').size()",
                "rate   = (failed / total * 100).round(2)",
                "result = pd.DataFrame({'failed_count': failed, 'total_count': total, 'failure_rate_%': rate}).sort_values('failure_rate_%', ascending=False)"
            )
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('device_type').size()",
            "total  = df.groupby('device_type').size()",
            "rate   = (failed / total * 100).round(2)",
            "result = pd.DataFrame({'Device': rate.index.tolist(), 'Failed': [int(failed[d]) for d in rate.index], 'Total': [int(total[d]) for d in rate.index], 'Failure_Rate_%': rate.values.tolist()}).sort_values('Failure_Rate_%', ascending=False).reset_index(drop=True)"
        )

    # â”€â”€ Bank most failed by COUNT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("most failed" in q or "most failures" in q or "highest failed" in q or "most fail" in q) and "bank" in q:
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('sender_bank').size().sort_values(ascending=False)",
            "total  = df.groupby('sender_bank').size()",
            "top_bank = str(failed.index[0])",
            "top_count = int(failed.iloc[0])",
            "top_total = int(total[failed.index[0]])",
            "top_rate = round(top_count/top_total*100, 2)",
            "ans = '=== DIRECT ANSWER: ' + top_bank + ' has the MOST failed transactions = ' + str(top_count) + ' failures (' + str(top_rate) + '% failure rate) ==='",
            "rows = pd.DataFrame([{'Bank': str(failed.index[i]), 'Failed_Count': int(failed.iloc[i]), 'Total': int(total[failed.index[i]]), 'Failure_Rate_%': round(int(failed.iloc[i])/int(total[failed.index[i]])*100,2)} for i in range(len(failed))])",
            "result = ans + chr(10) + chr(10) + rows.to_string(index=False)"
        )

    # â”€â”€ Bank + transaction type failure rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("failure rate" in q or "fail" in q) and "bank" in q and ("type" in q or "combination" in q):
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby(['sender_bank','transaction_type']).size()",
            "total  = df.groupby(['sender_bank','transaction_type']).size()",
            "rate   = (failed/total*100).round(2)",
            "tmp = pd.DataFrame({'failed_count': failed, 'total_count': total, 'failure_rate_%': rate}).sort_values('failure_rate_%', ascending=False).reset_index()",
            "tmp.columns = ['Bank','Transaction_Type','Failed_Count','Total_Count','Failure_Rate_%']",
            "result = tmp.head(10)"
        )

    # â”€â”€ Network failure rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("failure rate" in q or "fail" in q) and "network" in q:
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('network_type').size()",
            "total  = df.groupby('network_type').size()",
            "rate   = (failed / total * 100).round(2)",
            "result = pd.DataFrame({'Network': rate.index.tolist(), 'Failed': [int(failed[n]) for n in rate.index], 'Total': [int(total[n]) for n in rate.index], 'Failure_Rate_%': rate.values.tolist()}).sort_values('Failure_Rate_%', ascending=False).reset_index(drop=True)"
        )

    # â”€â”€ Merchant category failure rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("failure rate" in q or "fail" in q) and ("merchant" in q or "categor" in q):
        wkend = "is_weekend==1" if "weekend" in q else ("is_weekend==0" if "weekday" in q else None)
        if wkend:
            col = wkend.split('==')[0]
            val = wkend.split('==')[1]
            return code(
                "df_f = df[df['" + col + "']==" + val + "]",
                "failed = df_f[df_f['transaction_status']=='FAILED'].groupby('merchant_category').size()",
                "total  = df_f.groupby('merchant_category').size()",
                "result = (failed / total * 100).round(2).sort_values(ascending=False).to_dict()"
            )
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('merchant_category').size()",
            "total  = df.groupby('merchant_category').size()",
            "result = (failed / total * 100).round(2).sort_values(ascending=False).dropna().to_dict()"
        )

    # â”€â”€ Bank failure rate for weekend (context-follow-up) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("fail" in q or "failure" in q) and "bank" in q and "weekend" in q:
        return code(
            "wk = df[df['is_weekend']==1]",
            "failed = wk[wk['transaction_status']=='FAILED'].groupby('sender_bank').size()",
            "total  = wk.groupby('sender_bank').size()",
            "rate   = (failed / total * 100).round(2)",
            "result = pd.DataFrame({'Bank': rate.index.tolist(), 'Failed': failed.values.tolist(), 'Total': total.values.tolist(), 'Failure_Rate_%': rate.values.tolist()}).sort_values('Failure_Rate_%', ascending=False).reset_index(drop=True)"
        )

    # â”€â”€ Bank failure rate for iOS (context-follow-up) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("fail" in q or "failure" in q) and "bank" in q and "ios" in q:
        return code(
            "ios = df[df['device_type']=='iOS']",
            "failed = ios[ios['transaction_status']=='FAILED'].groupby('sender_bank').size()",
            "total  = ios.groupby('sender_bank').size()",
            "rate   = (failed / total * 100).round(2)",
            "result = pd.DataFrame({'Bank': rate.index.tolist(), 'Failed': failed.values.tolist(), 'Total': total.values.tolist(), 'Failure_Rate_%': rate.values.tolist()}).sort_values('Failure_Rate_%', ascending=False).reset_index(drop=True)"
        )

    # â”€â”€ Transaction type failure rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("failure rate" in q or "fail" in q) and ("transaction type" in q or "all" in q or ("compare" in q and "transaction" in q)):
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('transaction_type').size()",
            "total  = df.groupby('transaction_type').size()",
            "rate   = (failed / total * 100).round(2).sort_values(ascending=False)",
            "rows = pd.DataFrame({'Type': rate.index, 'Failed': [int(failed[t]) for t in rate.index], 'Total': [int(total[t]) for t in rate.index], 'Failure_Rate_%': rate.values})",
            "top_type = str(rows['Type'].iloc[0])",
            "top_rate = rows['Failure_Rate_%'].iloc[0]",
            "bot_type = str(rows['Type'].iloc[-1])",
            "bot_rate = rows['Failure_Rate_%'].iloc[-1]",
            "ans = '=== HIGHEST: ' + top_type + ' at ' + str(top_rate) + '% === LOWEST: ' + bot_type + ' at ' + str(bot_rate) + '% ==='",
            "result = ans + chr(10) + chr(10) + rows.to_string(index=False)"
        )


    # â”€â”€ State failure rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("failure rate" in q or "fail" in q) and "state" in q:
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('sender_state').size()",
            "total  = df.groupby('sender_state').size()",
            "rate   = (failed / total * 100).round(2)",
            "result = pd.DataFrame({'failed_count': failed, 'total_count': total, 'failure_rate_%': rate}).sort_values('failure_rate_%', ascending=False)"
        )

    # â”€â”€ Age group failure rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("failure rate" in q or "fail" in q) and "age" in q:
        return code(
            "grp = df.groupby('sender_age_group')",
            "failed = grp.apply(lambda x: (x['transaction_status']=='FAILED').sum())",
            "total  = grp.size()",
            "rate   = (failed / total * 100).round(4)",
            "result = pd.DataFrame({'failed_count': failed, 'total_count': total, 'failure_rate_%': rate}).sort_values('failure_rate_%', ascending=False)"
        )

    # Weekend vs weekday now handled earlier in router

    # â”€â”€ Weekend vs weekday failure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("weekend" in q and "weekday" in q) or ("weekend" in q and ("compare" in q or "versus" in q or "vs" in q or "fail" in q)):
        return code(
            "wkend = df[df['is_weekend']==1]",
            "wkday = df[df['is_weekend']==0]",
            "result = pd.DataFrame({",
            "    'Period': ['Weekend','Weekday'],",
            "    'Total_Transactions': [len(wkend), len(wkday)],",
            "    'Failed': [int(wkend['transaction_status'].eq('FAILED').sum()), int(wkday['transaction_status'].eq('FAILED').sum())],",
            "    'Failure_Rate_%': [round(wkend['transaction_status'].eq('FAILED').mean()*100,2), round(wkday['transaction_status'].eq('FAILED').mean()*100,2)],",
            "    'Avg_Amount_INR': [round(wkend['amount_inr'].mean(),2), round(wkday['amount_inr'].mean(),2)]",
            "})"
        )

    # â”€â”€ Weekend FRAUD by age (must be before spending pattern) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("fraud" in q or "flag" in q) and "age" in q and "weekend" in q:
        return code(
            "wknd_df = df[df['is_weekend']==1]",
            "fraud_wk = wknd_df.groupby('sender_age_group')['fraud_flag'].agg(['sum','count'])",
            "fraud_wk.columns = ['Fraud_Count','Total_Count']",
            "fraud_wk['Fraud_Rate_%'] = (fraud_wk['Fraud_Count']/fraud_wk['Total_Count']*100).round(4)",
            "fraud_wk = fraud_wk.sort_values('Fraud_Rate_%', ascending=False).reset_index()",
            "fraud_wk.rename(columns={'sender_age_group':'Age_Group'}, inplace=True)",
            "result = fraud_wk"
        )

    # â”€â”€ Weekend spending by age â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "weekend" in q and ("age" in q or "spend" in q or "amount" in q):
        return code(
            "result = df[df['is_weekend']==1].groupby('sender_age_group')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()"
        )

    # â”€â”€ Fraud by bank â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("fraud" in q or "flag" in q) and "bank" in q:
        return code(
            "tmp = df.groupby('sender_bank')['fraud_flag'].agg(['sum','count']).copy()",
            "tmp.columns = ['Fraud_Count','Total_Count']",
            "tmp['Fraud_Rate_%'] = (tmp['Fraud_Count']/tmp['Total_Count']*100).round(4)",
            "tmp = tmp.sort_values('Fraud_Rate_%', ascending=False).reset_index().rename(columns={'sender_bank':'Bank'})",
            "top_fb = str(tmp['Bank'].iloc[0])", 
            "top_fr = tmp['Fraud_Rate_%'].iloc[0]", 
            "top_fc = int(tmp['Fraud_Count'].iloc[0])", 
            "top_ft = int(tmp['Total_Count'].iloc[0])", 
            "bot_fb = str(tmp['Bank'].iloc[-1])", 
            "bot_fr = tmp['Fraud_Rate_%'].iloc[-1]", 
            "summary = '=== HIGHEST fraud rate: ' + top_fb + ' at ' + str(top_fr) + '% (' + str(top_fc) + ' of ' + str(top_ft) + '). LOWEST: ' + bot_fb + ' at ' + str(bot_fr) + '% ==='",
            "result = summary + chr(10) + chr(10) + tmp.to_string(index=False)"
        )

    # â”€â”€ Fraud by network â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("fraud" in q or "flag" in q) and "network" in q:
        return code(
            "result = df[df['fraud_flag']==1].groupby('network_type').size().sort_values(ascending=False).to_dict()"
        )

    # â”€â”€ High value fraud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("fraud" in q or "flag" in q) and ("high value" in q or "above" in q or "5000" in q or "percent" in q or "%" in q) and ("5000" in q or "high value" in q or "above" in q):
        return code(
            "high_val = df[df['amount_inr'] > 5000]",
            "total_high = len(high_val)",
            "flagged_high = int(high_val['fraud_flag'].sum())",
            "rate = round(high_val['fraud_flag'].mean() * 100, 4)",
            "overall_rate = round(df['fraud_flag'].mean()*100, 4)",
            "result = pd.DataFrame({'Metric': ['Total transactions above Rs5000', 'Flagged among those', 'Flag Rate above Rs5000', 'Overall flag rate all transactions'], 'Value': [total_high, flagged_high, str(rate)+'%', str(overall_rate)+'%']})"
        )

    # â”€â”€ Fraud by age â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("fraud" in q or "flag" in q) and "age" in q:
        return code(
            "result = df.groupby('sender_age_group')['fraud_flag'].mean().mul(100).round(2).sort_values(ascending=False).to_dict()"
        )

    # â”€â”€ Fraud by state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("fraud" in q or "flag" in q) and "state" in q:
        return code(
            "tmp = df.groupby('sender_state')['fraud_flag'].agg(['sum','count'])",
            "tmp.columns = ['Fraud_Count','Total_Count']",
            "tmp['Fraud_Rate_%'] = (tmp['Fraud_Count']/tmp['Total_Count']*100).round(4)",
            "tmp = tmp.sort_values('Fraud_Rate_%', ascending=False).reset_index().rename(columns={'sender_state':'State'})",
            "top_ss = str(tmp['State'].iloc[0])", 
            "top_sr = tmp['Fraud_Rate_%'].iloc[0]", 
            "top_sc = int(tmp['Fraud_Count'].iloc[0])", 
            "top_st = int(tmp['Total_Count'].iloc[0])", 
            "summary = '=== HIGHEST fraud rate by state: ' + top_ss + ' at ' + str(top_sr) + '% (' + str(top_sc) + ' of ' + str(top_st) + ') ==='",
            "result = summary + chr(10) + chr(10) + tmp.head(10).to_string(index=False)"
        )

    # â”€â”€ Peak hours â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("peak hour" in q or "hour" in q) and "fail" in q:
        return code(
            "result = df[df['transaction_status']=='FAILED'].groupby('hour_of_day').size().sort_values(ascending=False).to_dict()"
        )

    if "peak hour" in q and "p2m" in q:
        return code(
            "result = df[df['transaction_type']=='P2M'].groupby('hour_of_day').size().sort_values(ascending=False).to_dict()"
        )

    if "peak hour" in q or ("hour" in q and ("most" in q or "highest" in q)):
        return code(
            "hourly = df.groupby('hour_of_day').size().sort_values(ascending=False).reset_index()",
            "hourly.columns = ['Hour', 'Transaction_Count']",
            "hourly['Hour'] = hourly['Hour'].astype(str) + ':00'",
            "result = hourly"
        )

    # â”€â”€ Monthly trend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "month" in q and ("trend" in q or "volume" in q or "count" in q or "transactions" in q or "2024" in q):
        return code(
            "df2 = df.copy()",
            "df2['month'] = df2['timestamp'].dt.to_period('M').astype(str)",
            "monthly = df2.groupby('month').size().sort_index()",
            "highest = monthly.idxmax()",
            "lowest  = monthly.idxmin()",
            "result = (",
            "    f'>>> HIGHEST MONTH: {highest} with {int(monthly[highest]):,} transactions <<<\n'",
            "    f'>>> LOWEST MONTH: {lowest} with {int(monthly[lowest]):,} transactions <<<\n'",
            "    f'Average per month: {int(monthly.mean()):,}\n\n'",
            "    + '\n'.join([f'{m}: {int(v):,} transactions' for m, v in monthly.items()])",
            ")"
        )

    # â”€â”€ Day of week â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "day of week" in q or "day_of_week" in q or ("day" in q and ("week" in q or "highest" in q or "average" in q)):
        if "amount" in q or "spend" in q or "average" in q or "avg" in q:
            return code(
                "order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']",
                "result = df.groupby('day_of_week')['amount_inr'].mean().round(2).reindex(order).to_dict()"
            )
        return code(
            "order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']",
            "result = df.groupby('day_of_week').size().reindex(order).to_dict()"
        )

    # â”€â”€ Success rate comparisons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "success rate" in q and "network" in q:
        return code(
            "success = df[df['transaction_status']=='SUCCESS'].groupby('network_type').size()",
            "total   = df.groupby('network_type').size()",
            "result  = (success / total * 100).round(2).sort_values(ascending=False).to_dict()"
        )

    if "success rate" in q and "bank" in q:
        return code(
            "success = df[df['transaction_status']=='SUCCESS'].groupby('sender_bank').size()",
            "total   = df.groupby('sender_bank').size()",
            "result  = (success / total * 100).round(2).sort_values(ascending=False).to_dict()"
        )

    # â”€â”€ P2P by age group â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "p2p" in q and ("age" in q or "most" in q or "frequently" in q):
        return code(
            "result = df[df['transaction_type']=='P2P'].groupby('sender_age_group').size().sort_values(ascending=False).to_dict()"
        )

    # â”€â”€ Average amount by state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("average" in q or "avg" in q) and "state" in q:
        return code(
            "result = df.groupby('sender_state')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()"
        )

    # â”€â”€ Average amount for failed by bank â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if ("average" in q or "avg" in q) and "bank" in q and "fail" in q:
        return code(
            "result = df[df['transaction_status']=='FAILED'].groupby('sender_bank')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()"
        )

    # Average amount by merchant category
    if ("average" in q or "avg" in q) and ("merchant" in q or "categor" in q):
        return code(
            "result = df.groupby('merchant_category')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()"
        )

    # Average amount by state
    if ("average" in q or "avg" in q) and "state" in q:
        return code(
            "result = df.groupby('sender_state')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()"
        )

    # Average amount by age group
    if ("average" in q or "avg" in q) and "age" in q:
        return code(
            "result = df.groupby('sender_age_group')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()"
        )

    # Average amount by bank
    if ("average" in q or "avg" in q) and "bank" in q and "fail" not in q:
        return code(
            "result = df.groupby('sender_bank')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()"
        )

    # Average amount by transaction type
    if ("average" in q or "avg" in q) and ("transaction" in q or "type" in q) and "amount" in q:
        return code(
            "result = df.groupby('transaction_type')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()"
        )

    # Count/volume by state
    if ("count" in q or "volume" in q or "number" in q or "transactions" in q) and "state" in q:
        return code(
            "result = df.groupby('sender_state').size().sort_values(ascending=False).to_dict()"
        )

    # Count by transaction type
    if ("count" in q or "volume" in q or "number" in q) and ("transaction" in q or "type" in q):
        return code(
            "result = df.groupby('transaction_type').size().sort_values(ascending=False).to_dict()"
        )

    # Count by bank
    if ("count" in q or "volume" in q or "number" in q) and "bank" in q:
        return code(
            "result = df.groupby('sender_bank').size().sort_values(ascending=False).to_dict()"
        )

    # Just "pct" alone or "pct of fraud"
    if q.strip() in ["pct","percentage","percent"]:
        return code("result = round(df['fraud_flag'].mean() * 100, 2)")

    # Fraud percentage
    if ("pct" in q or "percent" in q or "percentage" in q or "rate" in q) and ("fraud" in q or "flag" in q) and not any(x in q for x in ["state","bank","age","network","device"]):
        return code(
            "result = round(df['fraud_flag'].mean() * 100, 2)"
        )

    # Deep dive / drill down â€” break down by multiple dimensions
    if any(phrase in q for phrase in ["deeper breakdown","deeper analysis","deep dive","drill down","break it down","breakdown of"]):
        if "fail" in q or "failure" in q:
            return code(
                "failed_dev = df[df['transaction_status']=='FAILED'].groupby('device_type').size()",
                "total_dev  = df.groupby('device_type').size()",
                "failed_net = df[df['transaction_status']=='FAILED'].groupby('network_type').size()",
                "total_net  = df.groupby('network_type').size()",
                "failed_age = df[df['transaction_status']=='FAILED'].groupby('sender_age_group').size()",
                "total_age  = df.groupby('sender_age_group').size()",
                "result = {",
                "    'By Device': (failed_dev/total_dev*100).round(2).to_dict(),",
                "    'By Network': (failed_net/total_net*100).round(2).to_dict(),",
                "    'By Age Group': (failed_age/total_age*100).round(2).to_dict(),",
                "}"
            )
        if "recharge" in q:
            return code(
                "df_r = df[df['transaction_type']=='Recharge']",
                "failed_dev = df_r[df_r['transaction_status']=='FAILED'].groupby('device_type').size()",
                "total_dev  = df_r.groupby('device_type').size()",
                "failed_net = df_r[df_r['transaction_status']=='FAILED'].groupby('network_type').size()",
                "total_net  = df_r.groupby('network_type').size()",
                "failed_bank = df_r[df_r['transaction_status']=='FAILED'].groupby('sender_bank').size()",
                "total_bank  = df_r.groupby('sender_bank').size()",
                "result = {",
                "    'Recharge Failure by Device': (failed_dev/total_dev*100).round(2).to_dict(),",
                "    'Recharge Failure by Network': (failed_net/total_net*100).round(2).to_dict(),",
                "    'Recharge Failure by Bank': (failed_bank/total_bank*100).round(2).to_dict(),",
                "}"
            )

    # "Why is X failing" questions - show failure rate comparison
    if "why" in q and ("fail" in q or "failing" in q):
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('transaction_type').size()",
            "total  = df.groupby('transaction_type').size()",
            "result = (failed / total * 100).round(2).sort_values(ascending=False).to_dict()"
        )

    # "Worst performance" / "best performance" by device, network, bank etc
    if any(phrase in q for phrase in ["worst performance","best performance","worst performing","best performing","poorly performing","worst device","best device"]):
        if "device" in q or "android" in q or "ios" in q or "web" in q:
            return code(
                "failed = df[df['transaction_status']=='FAILED'].groupby('device_type').size()",
                "total  = df.groupby('device_type').size()",
                "result = (failed / total * 100).round(2).sort_values(ascending=False).to_dict()"
            )
        elif "network" in q:
            return code(
                "failed = df[df['transaction_status']=='FAILED'].groupby('network_type').size()",
                "total  = df.groupby('network_type').size()",
                "result = (failed / total * 100).round(2).sort_values(ascending=False).to_dict()"
            )
        elif "bank" in q:
            return code(
                "failed = df[df['transaction_status']=='FAILED'].groupby('sender_bank').size()",
                "total  = df.groupby('sender_bank').size()",
                "result = (failed / total * 100).round(2).sort_values(ascending=False).to_dict()"
            )
        else:
            # Default â€” compare all dimensions
            return code(
                "failed_dev = df[df['transaction_status']=='FAILED'].groupby('device_type').size()",
                "total_dev  = df.groupby('device_type').size()",
                "failed_net = df[df['transaction_status']=='FAILED'].groupby('network_type').size()",
                "total_net  = df.groupby('network_type').size()",
                "result = {",
                "    'Failure by Device %': (failed_dev/total_dev*100).round(2).sort_values(ascending=False).to_dict(),",
                "    'Failure by Network %': (failed_net/total_net*100).round(2).sort_values(ascending=False).to_dict(),",
                "}"
            )

    # "Most at risk" age group
    if ("most at risk" in q or "highest risk" in q or "riskiest" in q) and "age" in q:
        return code(
            "result = {",
            "    'Fraud rate by age %': df.groupby('sender_age_group')['fraud_flag'].mean().mul(100).round(2).sort_values(ascending=False).to_dict(),",
            "    'Failure rate by age %': (df[df['transaction_status']=='FAILED'].groupby('sender_age_group').size() / df.groupby('sender_age_group').size()*100).round(2).sort_values(ascending=False).to_dict(),",
            "}"
        )

    # "Most important insight" / "key insight"
    if any(phrase in q for phrase in ["most important insight","key insight","biggest insight","main insight","top insight","most critical"]):
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('transaction_type').size()",
            "total  = df.groupby('transaction_type').size()",
            "fail_type = (failed/total*100).round(2).sort_values(ascending=False).to_dict()",
            "failed_net = df[df['transaction_status']=='FAILED'].groupby('network_type').size()",
            "total_net  = df.groupby('network_type').size()",
            "fail_net = (failed_net/total_net*100).round(2).sort_values(ascending=False).to_dict()",
            "result = {",
            "    'Overall Success Rate %': round((df['transaction_status']=='SUCCESS').mean()*100,2),",
            "    'Overall Failure Rate %': round((df['transaction_status']=='FAILED').mean()*100,2),",
            "    'Highest Failure Type': fail_type,",
            "    'Highest Failure Network': fail_net,",
            "    'Fraud Flag Rate %': round(df['fraud_flag'].mean()*100,2),",
            "    'Avg Transaction INR': round(df['amount_inr'].mean(),2),",
            "}"
        )

    # "Banks performing poorly" = banks with highest failure rate
    if ("perform" in q and "poor" in q and "bank" in q) or ("worst" in q and "bank" in q) or ("bad" in q and "bank" in q):
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('sender_bank').size()",
            "total  = df.groupby('sender_bank').size()",
            "rate   = (failed / total * 100).round(2)",
            "result = pd.DataFrame({'Bank': rate.index.tolist(), 'Failed': [int(failed[b]) for b in rate.index], 'Total': [int(total[b]) for b in rate.index], 'Failure_Rate_%': rate.values.tolist()}).sort_values('Failure_Rate_%', ascending=False).reset_index(drop=True)"
        )

    # "Risky states" = states with highest fraud flag rate
    if ("risk" in q or "risky" in q or "dangerous" in q or "unsafe" in q) and "state" in q:
        return code(
            "result = df.groupby('sender_state')['fraud_flag'].mean().mul(100).round(2).sort_values(ascending=False).to_dict()"
        )

    # "Reduce fraud" or "fraud risk profile" or "most at risk fraud"
    if ("reduce" in q or "prevent" in q or "stop" in q or "risk profile" in q or "most at risk" in q) and ("fraud" in q or "flag" in q):
        return code(
            "total_txns = len(df)",
            "total_fraud = int(df['fraud_flag'].sum())",
            "overall_rate = round(df['fraud_flag'].mean()*100, 4)",
            "# Age group fraud - ranked by rate",
            "fraud_age = df.groupby('sender_age_group')['fraud_flag'].agg(['sum','count']).rename(columns={'sum':'fraud_count','count':'total_count'})",
            "fraud_age['fraud_rate_%'] = (fraud_age['fraud_count']/fraud_age['total_count']*100).round(4)",
            "fraud_age = fraud_age.sort_values('fraud_rate_%', ascending=False)",
            "# State fraud - ranked by rate",
            "fraud_state = df.groupby('sender_state')['fraud_flag'].agg(['sum','count']).rename(columns={'sum':'fraud_count','count':'total_count'})",
            "fraud_state['fraud_rate_%'] = (fraud_state['fraud_count']/fraud_state['total_count']*100).round(4)",
            "fraud_state = fraud_state.sort_values('fraud_rate_%', ascending=False).head(5)",
            "# Bank fraud - ranked by rate",
            "fraud_bank = df.groupby('sender_bank')['fraud_flag'].agg(['sum','count']).rename(columns={'sum':'fraud_count','count':'total_count'})",
            "fraud_bank['fraud_rate_%'] = (fraud_bank['fraud_count']/fraud_bank['total_count']*100).round(4)",
            "fraud_bank = fraud_bank.sort_values('fraud_rate_%', ascending=False)",
            "result = {",
            "    'OVERALL: Total transactions': total_txns,",
            "    'OVERALL: Total flagged': total_fraud,",
            "    'OVERALL: Fraud rate %': overall_rate,",
            "    'HIGHEST RISK Age Group': fraud_age.index[0] + ' (' + str(fraud_age['fraud_rate_%'].iloc[0]) + '% â€” ' + str(int(fraud_age['fraud_count'].iloc[0])) + ' of ' + str(int(fraud_age['total_count'].iloc[0])) + ')',",
            "    'HIGHEST RISK State': fraud_state.index[0] + ' (' + str(fraud_state['fraud_rate_%'].iloc[0]) + '% â€” ' + str(int(fraud_state['fraud_count'].iloc[0])) + ' of ' + str(int(fraud_state['total_count'].iloc[0])) + ')',",
            "    'HIGHEST RISK Bank': fraud_bank.index[0] + ' (' + str(fraud_bank['fraud_rate_%'].iloc[0]) + '% â€” ' + str(int(fraud_bank['fraud_count'].iloc[0])) + ' of ' + str(int(fraud_bank['total_count'].iloc[0])) + ')',",
            "    'All age groups ranked': fraud_age[['fraud_count','total_count','fraud_rate_%']].to_dict(),",
            "    'Top 5 states ranked': fraud_state[['fraud_count','total_count','fraud_rate_%']].to_dict(),",
            "    'All banks ranked': fraud_bank[['fraud_count','total_count','fraud_rate_%']].to_dict(),",
            "}"
        )

    # "Complete diagnosis" / "overall health" / "system overview"
    if any(phrase in q for phrase in ["complete diagnosis","full diagnosis","system diagnosis","payment diagnosis",
        "overall health","system health","complete analysis","full analysis","overall performance",
        "how is our system","how are we performing","complete overview",
        "how are payments doing","payments doing","how is it going","overall summary","quick summary",
        "how are we doing overall","payments overall"]):
        return code(
            "failed = df[df['transaction_status']=='FAILED'].groupby('transaction_type').size()",
            "total  = df.groupby('transaction_type').size()",
            "fail_by_type = (failed/total*100).round(2).sort_values(ascending=False).to_dict()",
            "failed_dev = df[df['transaction_status']=='FAILED'].groupby('device_type').size()",
            "total_dev  = df.groupby('device_type').size()",
            "fail_by_dev = (failed_dev/total_dev*100).round(2).sort_values(ascending=False).to_dict()",
            "failed_net = df[df['transaction_status']=='FAILED'].groupby('network_type').size()",
            "total_net  = df.groupby('network_type').size()",
            "fail_by_net = (failed_net/total_net*100).round(2).sort_values(ascending=False).to_dict()",
            "failed_bank = df[df['transaction_status']=='FAILED'].groupby('sender_bank').size()",
            "total_bank  = df.groupby('sender_bank').size()",
            "fail_by_bank = (failed_bank/total_bank*100).round(2).sort_values(ascending=False).to_dict()",
            "result = {",
            "    'âœ… Overall Success Rate %': round((df['transaction_status']=='SUCCESS').mean()*100,2),",
            "    'âŒ Overall Failure Rate %': round((df['transaction_status']=='FAILED').mean()*100,2),",
            "    'âš ï¸ Fraud Flag Rate %': round(df['fraud_flag'].mean()*100,2),",
            "    'ðŸ’³ Avg Transaction â‚¹': round(df['amount_inr'].mean(),2),",
            "    'âŒ Failure by Transaction Type': fail_by_type,",
            "    'âŒ Failure by Device': fail_by_dev,",
            "    'âŒ Failure by Network': fail_by_net,",
            "    'âŒ Failure by Bank': fail_by_bank,",
            "}"
        )

    # "Are weekends better or worse"
    if "weekend" in q and ("better" in q or "worse" in q or "good" in q or "bad" in q):
        return code(
            "result = {",
            "    'Weekend failure rate %': round(df[df['is_weekend']==1]['transaction_status'].eq('FAILED').mean()*100,2),",
            "    'Weekday failure rate %': round(df[df['is_weekend']==0]['transaction_status'].eq('FAILED').mean()*100,2),",
            "    'Weekend avg amount â‚¹': round(df[df['is_weekend']==1]['amount_inr'].mean(),2),",
            "    'Weekday avg amount â‚¹': round(df[df['is_weekend']==0]['amount_inr'].mean(),2),",
            "}"
        )

    # "Tell me about [age group]"
    if "tell me" in q and "age" in q:
        return code(
            "result = {",
            "    'Transaction count by age': df.groupby('sender_age_group').size().to_dict(),",
            "    'Avg amount by age â‚¹': df.groupby('sender_age_group')['amount_inr'].mean().round(2).to_dict(),",
            "    'Failure rate by age %': (df[df['transaction_status']=='FAILED'].groupby('sender_age_group').size() / df.groupby('sender_age_group').size()*100).round(2).to_dict(),",
            "    'Fraud rate by age %': df.groupby('sender_age_group')['fraud_flag'].mean().mul(100).round(2).to_dict(),",
            "}"
        )

    return None


# â”€â”€ LLM: Generate Pandas Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_pandas_code(user_query, df_summary, conversation_history, client):
    system_prompt = """You are a senior Python/Pandas data analyst.
You have a DataFrame called `df` with this schema and summary:
DF_SUMMARY_PLACEHOLDER

EXACT column names:
- transaction_id, timestamp (datetime), transaction_type (P2P/P2M/Bill Payment/Recharge)
- merchant_category (Food/Grocery/Fuel/Entertainment/Shopping/Healthcare/Education/Transport/Utilities/Other)
- amount_inr (float), transaction_status (SUCCESS or FAILED)
- sender_age_group (18-25/26-35/36-45/46-55/56+), receiver_age_group
- sender_state (Indian state), sender_bank (SBI/HDFC/ICICI/Axis/PNB/Kotak/IndusInd/Yes Bank), receiver_bank
- device_type: EXACT values are "Android", "iOS", "Web" â€” no other values exist
- network_type: EXACT values are "4G", "5G", "WiFi", "3G" â€” no other values exist (NOT GPRS, NOT CDMA, NOT UMTS)
- fraud_flag (0 or 1), hour_of_day (0-23), day_of_week (Monday-Sunday), is_weekend (0 or 1)

STRICT RULES:
1. Write ONLY executable Python/Pandas code
2. Store final answer in variable `result`
3. result must be string, number, dict, or DataFrame â€” never a raw Series
4. Do NOT use print(), plt.show(), or any import statements
5. Return ONLY raw code â€” no markdown fences, no explanation
6. NEVER use groupby().apply(lambda) â€” broken in Pandas 2.x
7. ALWAYS filter binary cols with ==1: df[df['is_weekend']==1], df[df['fraud_flag']==1]

SAFE PATTERNS â€” always use these exact patterns:

# Failure rate comparison (ALWAYS return DataFrame sorted by rate descending):
failed = df[df['transaction_status']=='FAILED'].groupby('COLUMN').size()
total  = df.groupby('COLUMN').size()
rate   = (failed / total * 100).round(2)
result = pd.DataFrame({'Category': failed.index, 'Failed': failed.values, 'Total': total.values, 'Failure_Rate_%': rate.values}).sort_values('Failure_Rate_%', ascending=False).reset_index(drop=True)

# Fraud rate comparison (ALWAYS return DataFrame sorted by rate descending):
tmp = df.groupby('COLUMN')['fraud_flag'].agg(['sum','count'])
tmp.columns = ['Fraud_Count','Total']
tmp['Fraud_Rate_%'] = (tmp['Fraud_Count']/tmp['Total']*100).round(4)
result = tmp.sort_values('Fraud_Rate_%', ascending=False).reset_index()

# Average amount per group (sorted highest first):
result = df.groupby('COLUMN')['amount_inr'].mean().round(2).sort_values(ascending=False).reset_index()
result.columns = ['Category', 'Avg_Amount_INR']

# Count per group (sorted highest first):
result = df.groupby('COLUMN').size().sort_values(ascending=False).reset_index()
result.columns = ['Category', 'Count']

CRITICAL: ALWAYS sort descending. ALWAYS use DataFrame not dict. First row = highest value.

# Weekend transactions:
result = df[df['is_weekend']==1].groupby('COLUMN')['amount_inr'].mean().round(2).to_dict()

# Monthly trend:
df2 = df.copy()
df2['month'] = df2['timestamp'].dt.to_period('M').astype(str)
result = df2.groupby('month').size().to_dict()

# Peak hour for a transaction type:
result = df[df['transaction_type']=='P2M'].groupby('hour_of_day').size().idxmax()

# Fraud analysis:
result = df[df['fraud_flag']==1].groupby('COLUMN').size().sort_values(ascending=False).to_dict()

# Multi-column comparison (e.g. device vs transaction type):
failed = df[df['transaction_status']=='FAILED'].groupby(['device_type','transaction_type']).size()
total  = df.groupby(['device_type','transaction_type']).size()
result = (failed/total*100).round(2).reset_index(name='failure_rate')

# Age group spending on weekends:
result = df[df['is_weekend']==1].groupby('sender_age_group')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()

# Success rate comparison:
success = df[df['transaction_status']=='SUCCESS'].groupby('COLUMN').size()
total   = df.groupby('COLUMN').size()
result  = (success / total * 100).round(2).sort_values(ascending=False).to_dict()

# High value transactions flagged:
result = round(df[df['amount_inr'] > df['amount_inr'].quantile(0.9)]['fraud_flag'].mean() * 100, 2)

# Weekday vs weekend comparison:
result = df.groupby('is_weekend')['amount_inr'].mean().round(2).to_dict()

# Multi-filter then groupby (e.g. P2P only, compare Android vs iOS failure rate):
df_filtered = df[df['transaction_type']=='P2P']
failed = df_filtered[df_filtered['transaction_status']=='FAILED'].groupby('device_type').size()
total  = df_filtered.groupby('device_type').size()
result = (failed / total * 100).round(2).sort_values(ascending=False).to_dict()

# Age group fraud rate on weekends:
df_filtered = df[df['is_weekend']==1]
result = df_filtered.groupby('sender_age_group')['fraud_flag'].mean().mul(100).round(2).sort_values(ascending=False).to_dict()

# Failed transactions only â€” average amount by bank:
result = df[df['transaction_status']=='FAILED'].groupby('sender_bank')['amount_inr'].mean().round(2).sort_values(ascending=False).to_dict()

# Failure rate on weekdays only per merchant category:
df_filtered = df[df['is_weekend']==0]
failed = df_filtered[df_filtered['transaction_status']=='FAILED'].groupby('merchant_category').size()
total  = df_filtered.groupby('merchant_category').size()
result = (failed / total * 100).round(2).sort_values(ascending=False).to_dict()

# Transactions above a threshold flagged for fraud:
threshold = df['amount_inr'].quantile(0.9)
result = round(df[df['amount_inr'] > threshold]['fraud_flag'].mean() * 100, 2)

# Bank + transaction type combination failure rate:
failed = df[df['transaction_status']=='FAILED'].groupby(['sender_bank','transaction_type']).size()
total  = df.groupby(['sender_bank','transaction_type']).size()
result = (failed/total*100).round(2).sort_values(ascending=False).reset_index(name='failure_rate')
"""

    system_prompt = system_prompt.replace("DF_SUMMARY_PLACEHOLDER", df_summary)
    messages = [{"role": "system", "content": system_prompt}]
    for msg in conversation_history[-4:]:
        messages.append({"role": msg["role"], "content": msg["content"]})
    messages.append({"role": "user", "content": f"Write pandas code to answer: {user_query}"})

    response = client.chat.completions.create(
        model="local-model",
        messages=messages,
        temperature=0.1,
        max_tokens=600,
    )

    code = response.choices[0].message.content.strip()
    code = re.sub(r"```python\n?", "", code)
    code = re.sub(r"```\n?", "", code)
    return code.strip()

# â”€â”€ Execute Code Safely â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def execute_code(code, df):
    local_vars = {"df": df.copy(), "pd": pd}
    try:
        exec(code, {}, local_vars)
        result = local_vars.get("result", "No result variable found.")
        if isinstance(result, pd.DataFrame):
            return result.to_string(), None
        elif isinstance(result, dict):
            return json.dumps(result, indent=2, default=str), None
        else:
            return str(result), None
    except Exception as e:
        return None, str(e)

# â”€â”€ LLM: Natural Language Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_insight(user_query, data_result, error, conversation_history, client):
    system_prompt = """You are InsightX, a friendly and expert business intelligence assistant for a digital payments company.
Your job is to turn raw data into clear, explainable, actionable insights for business leaders.

DETECT QUESTION TYPE first, then respond appropriately:

TYPE A â€” Data questions ("which", "what is", "compare", "show me"):
RESPONSE STRUCTURE:
1. ðŸ“Š DIRECT ANSWER â€” one clear sentence with the key finding and number
2. ðŸ“‹ FULL DATA BREAKDOWN â€” ranked list of ALL values with exact numbers from the data:
   ðŸ¥‡ Recharge: 5.09%  (6,341 failed of 124,651 total)
   ðŸ¥ˆ P2P: 4.96%  (5,574 failed of 112,445 total)
3. ðŸ” HOW THIS WAS COMPUTED â€” one sentence explaining methodology naturally
4. ðŸ’¼ BUSINESS CONTEXT â€” 1-2 sentences of genuine business interpretation
5. ðŸ’¡ FOLLOW-UP â€” one genuinely useful next question

TYPE B â€” Explanatory questions ("how did you calculate", "explain", "walk me through", "why does", "what does X mean", "step by step", "limitations", "in simple terms"):
RESPONSE STRUCTURE:
1. ðŸ” METHODOLOGY â€” explain exactly how the calculation works step by step
2. ðŸ“Š THE NUMBERS â€” reference specific numbers from the data to illustrate
3. ðŸ’¡ WHAT THIS MEANS â€” plain English interpretation for a business leader
4. âš ï¸ LIMITATIONS â€” be honest about what this data can and cannot tell us

TYPE C â€” Strategic/Innovation questions ("how would you build", "design a", "what ML model", "scale to", "production-ready", "CEO dashboard"):
RESPONSE STRUCTURE:
1. ðŸŽ¯ DIRECT RECOMMENDATION â€” clear answer in one sentence
2. ðŸ—ï¸ APPROACH â€” 3-4 concrete steps or components
3. ðŸ“Š DATA EVIDENCE â€” reference specific numbers from InsightX data to justify
4. ðŸ’¼ BUSINESS IMPACT â€” expected outcome if implemented

NATURALNESS RULES:
- Vary your opening â€” don't always start with "Based on our analysis"
- Use conversational transitions: "Interestingly...", "Worth noting...", "The standout here is..."
- When a result is surprising flag it naturally: "This one might surprise you â€”"
- When result confirms expectations say: "As expected, ..." or "This aligns with industry trends â€”"
- Never sound like you're reading from a template
- Match tone to question â€” casual questions get warmer responses, technical questions get precise responses
- For simple single-value answers, skip the full structure and just answer naturally in 2-3 sentences

RULES:
- ALWAYS include sample sizes where possible e.g. "5.09% (6,341 out of 124,651 transactions)"
- ALWAYS show the methodology line â€” judges and analysts need to know how numbers were derived
- Use â‚¹ for rupee amounts
- Never say "I apologize" more than once
- If data is unavailable, give the closest useful alternative
- If result seems surprising, flag it: "âš ï¸ This is higher than the industry average of ~3-4% â€” worth investigating."
- Keep total response under 12 sentences
"""
    # Detect if this is an explanatory/strategic question (no data needed)
    explain_phrases_ins = [
        "how did you calculate", "how did you determine", "walk me through",
        "explain how", "explain why", "explain the difference",
        "how confident", "limitations of", "in simple terms", "step by step",
        "why does education", "why does 3g", "how would you", "design a",
        "build a risk score", "which 3 metrics", "what machine learning",
        "production-ready", "scale to", "if failure rate crosses",
        "fraud patterns are shifting", "what additional data",
        "what would a production", "ceo dashboard",
        "how is the average", "what does it tell us",
        "what does a 0", "actually mean in real", "mean in real numbers",
        "summarise everything", "summarize everything", "summarise what",
        "discussed so far", "we've discussed", "weve discussed",
        "which one should", "focus on fixing", "fix first",
    ]
    is_explain_q = any(p in user_query.lower() for p in explain_phrases_ins)

    if error and not is_explain_q:
        content = f"""User asked: {user_query}
Error encountered: {error}

Instructions: Don't just say error. Instead:
1. Acknowledge briefly (one sentence max)
2. Explain the limitation clearly
3. Provide the closest useful alternative insight you can
4. Suggest a rephrased question that would work"""
    elif is_explain_q or (data_result is None and not error):
        # If pandas ran and returned real data, INCLUDE it so LLM uses real numbers
        data_section = ""
        if data_result:
            data_section = f"""
REAL DATA (computed live from 250,000 transactions â€” USE THESE EXACT NUMBERS, do not invent your own):
{data_result}

CRITICAL: Every number you quote MUST come from the REAL DATA above.
Do NOT use numbers from memory or training data. Row 1 = highest value."""

        # For summarise questions, build conversation recap
        conv_summary = ""
        if any(p in user_query.lower() for p in ["summarise","summarize","discussed","so far"]):
            qa_pairs = []
            msgs = conversation_history[-20:]  # last 20 messages
            for i in range(0, len(msgs)-1, 2):
                if msgs[i]["role"] == "user" and i+1 < len(msgs) and msgs[i+1]["role"] == "assistant":
                    q_text = msgs[i]["content"][:80]
                    a_text = msgs[i+1]["content"][:200].replace("\n"," ")
                    qa_pairs.append(f"Q: {q_text}\nA: {a_text}...")
            if qa_pairs:
                conv_summary = "\n\nCONVERSATION HISTORY TO SUMMARISE:\n" + "\n---\n".join(qa_pairs)
                conv_summary += """\n\nNow write a clear CONVERSATION SUMMARY covering every topic discussed.
CRITICAL RULES for the summary:
- Failure rates are always 4-6% (like 5.1%, 4.95%)
- Fraud rates are always <1% (like 0.19%, 0.25%)  
- NEVER label a failure rate as a fraud rate or vice versa
- Reference specific banks, numbers and findings from the conversation
- Structure: what we found about banks â†’ iOS â†’ weekends â†’ fraud â†’ key recommendation"""

        content = f"""User asked: {user_query}

BACKGROUND â€” InsightX analyses 250,000 UPI transactions:
- Overall failure rate: 4.95% | Fraud flag rate: 0.19% | Avg amount: â‚¹1,312
- Transaction types: P2P (45%), P2M (30%), Bill Payment (15%), Recharge (10%)
- Network types: 4G (60%), 5G (25%), WiFi (10%), 3G (5%)
- Device: Android (75%), iOS (20%), Web (5%)
{data_section}{conv_summary}
This is a TYPE B explanatory question. Explain the methodology clearly AND use the real numbers above.
Never invent numbers â€” if real data is provided above, use only those numbers."""
    else:
        content = f"""User asked: {user_query}

DATA (computed directly from 250,000 UPI transactions â€” trust this completely):
{data_result}

STRICT READING RULES:
1. The data above is already SORTED â€” row 0 / first row = HIGHEST value. Use it as your direct answer.
2. NEVER re-sort, re-rank, or re-interpret the data. Read it exactly as given.
3. If data has columns like Failure_Rate_%, Fraud_Rate_%, read THAT column â€” not count columns.
4. Use ONLY numbers from the data â€” never invent, estimate, or recall from memory.
5. Fraud rates are always < 1%. Failure rates are always 4-6%. If you see otherwise, re-read the data.
6. Show ALL rows in the breakdown, ranked exactly as they appear in the data.
7. Format: "Name: X failed of Y total (Z%)" using exact numbers from the data.
8. Preserve decimal places exactly â€” 0.2496% stays 0.2496%, never round to 0.25% or 0.2%.
Generate a clear business insight. The direct answer = the FIRST ROW of the data."""

    messages = [{"role": "system", "content": system_prompt}]
    for msg in conversation_history[-4:]:
        messages.append({"role": msg["role"], "content": msg["content"]})
    messages.append({"role": "user", "content": content})

    # More tokens for multi-question responses
    is_multi = "MULTI-QUESTION" in user_query
    response = client.chat.completions.create(
        model="local-model",
        messages=messages,
        temperature=0.5,
        max_tokens=1200 if is_multi else 500,
    )
    return response.choices[0].message.content.strip()


# â”€â”€ Multi-part Question Splitter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def split_multipart(query):
    """Split multi-part questions into individual queries."""
    import re
    q = query.lower()

    # Never split follow-up/conversational queries
    no_split_phrases = [
        "tell me more", "more about", "what about", "same but",
        "now show", "why do you", "why is that", "go back",
        "summarise", "summarize", "which one should", "focus on",
        "interesting", "elaborate", "dig deeper", "compare that",
        "can you compare",
    ]
    if any(p in q for p in no_split_phrases):
        return [query]

    # Pattern 1: Numbered questions "1. X 2. Y 3. Z"
    numbered = re.split(r'\s*\d+[\.\):]\s+', query)
    numbered = [p.strip() for p in numbered if len(p.strip()) > 8]
    if len(numbered) > 1:
        return numbered

    # Pattern 2: "and also" / "also tell me"
    separators = [" and also tell me ", " and also ", " also tell me ", " additionally "]
    for sep in separators:
        if sep in q:
            parts = query.lower().split(sep)
            return [p.strip() for p in parts if len(p.strip()) > 5]

    return [query]  # single question

# â”€â”€ Smart Recommendations Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_recommendations(user_query, data_result):
    """
    Returns 2-3 actionable business recommendations based on query topic and data result.
    These are pattern-based recommendations triggered by keywords.
    """
    q = user_query.lower()
    recs = []

    # â”€â”€ Failure rate recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "fail" in q or "failure" in q:

        if "recharge" in q or ("recharge" in str(data_result)):
            recs = [
                "ðŸ’¡ **Implement retry logic** â€” Add automatic retry (up to 3 attempts) for failed recharge transactions, as many failures are due to temporary network timeouts rather than permanent errors.",
                "ðŸ’¡ **Partner API monitoring** â€” Set up real-time monitoring for recharge partner APIs (Airtel, Jio, BSNL etc.) with automatic failover to backup providers when failure rates exceed 5%.",
                "ðŸ’¡ **User notification system** â€” Send instant SMS/push notifications on failure with a one-tap retry button, reducing customer frustration and improving recovery rate."
            ]

        elif "network" in q or "3g" in str(data_result).lower() or "4g" in str(data_result).lower():
            recs = [
                "ðŸ’¡ **Adaptive timeout settings** â€” Increase transaction timeout thresholds for 3G/slow network users from the standard 30s to 60s to reduce premature timeouts.",
                "ðŸ’¡ **Offline queue mechanism** â€” Allow transactions to be queued locally when network is poor and auto-submit when connectivity improves.",
                "ðŸ’¡ **Network-aware UX** â€” Detect connection quality before initiating payment and warn users on poor networks: 'Your connection is slow â€” transaction may take longer.'"
            ]

        elif "device" in q or "web" in str(data_result).lower() or "android" in str(data_result).lower():
            recs = [
                "ðŸ’¡ **Web gateway audit** â€” Conduct a technical audit of the web payment gateway integration â€” web typically has higher failures due to browser compatibility and session timeout issues.",
                "ðŸ’¡ **Device-specific testing** â€” Run automated payment flow tests across all device/browser combinations weekly to catch regressions early.",
                "ðŸ’¡ **Progressive Web App (PWA)** â€” Convert the web payment flow to a PWA for better reliability, offline support, and a native-app-like experience."
            ]

        elif "bank" in q:
            recs = [
                "ðŸ’¡ **Bank partner SLA review** â€” Initiate quarterly SLA reviews with high-failure banks and set contractual uptime requirements with penalty clauses.",
                "ðŸ’¡ **Smart bank routing** â€” Implement dynamic routing that automatically switches to backup bank APIs when a primary bank's failure rate exceeds threshold.",
                "ðŸ’¡ **Bank health dashboard** â€” Build an internal real-time dashboard tracking each bank partner's API response time and failure rate for proactive intervention."
            ]

        elif "bill payment" in q or "bill" in q:
            recs = [
                "ðŸ’¡ **Biller validation pre-check** â€” Before initiating payment, validate the bill account number with the biller API to catch errors upfront instead of at payment time.",
                "ðŸ’¡ **Scheduled bill payments** â€” Offer customers the ability to schedule bill payments during off-peak hours (2-6 AM) when biller systems are less loaded.",
                "ðŸ’¡ **Multi-biller fallback** â€” Integrate with 2+ biller aggregators (BBPS, BillDesk) so if one fails, payment automatically routes through the alternate."
            ]

        elif "age" in q:
            recs = [
                "ðŸ’¡ **Simplified UX for 56+ users** â€” Redesign payment flow with larger buttons, simpler steps, and confirmation screens to reduce user-error-driven failures in older age groups.",
                "ðŸ’¡ **Assisted payment feature** â€” Add a 'help me pay' option that walks first-time/older users through the payment process step by step.",
                "ðŸ’¡ **Targeted onboarding** â€” Run digital literacy campaigns for the 56+ segment with video tutorials on completing transactions successfully."
            ]

        else:
            recs = [
                "ðŸ’¡ **Root cause classification** â€” Categorise failures into: network errors, bank errors, user errors, and system errors to prioritise fixes by volume and impact.",
                "ðŸ’¡ **Real-time failure alerting** â€” Set up automated alerts when failure rate for any segment exceeds 6% so the ops team can respond within minutes.",
                "ðŸ’¡ **Failure recovery flow** â€” Design a dedicated failure recovery UX that guides users through what went wrong and offers instant retry, alternative payment method, or customer support."
            ]

    # â”€â”€ Fraud recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif "fraud" in q or "flag" in q:

        if "state" in q:
            recs = [
                "ðŸ’¡ **Geo-based risk scoring** â€” Add location-based risk scores to the fraud detection model, applying stricter verification for high-fraud states.",
                "ðŸ’¡ **Regional fraud task force** â€” Deploy dedicated fraud investigation teams in top-fraud states to analyse patterns and work with local law enforcement.",
                "ðŸ’¡ **Step-up authentication** â€” Require additional OTP or biometric verification for transactions originating from high-fraud regions."
            ]

        elif "age" in q:
            recs = [
                "ðŸ’¡ **Behavioural analytics** â€” Deploy ML-based behavioural analytics to detect unusual transaction patterns for high-risk age groups (18-25, 56+).",
                "ðŸ’¡ **Education campaigns** â€” Run targeted fraud awareness campaigns for the 18-25 segment on social media explaining common UPI scams.",
                "ðŸ’¡ **Transaction limits** â€” Implement daily transaction limits for new accounts in high-fraud age groups until they build a trusted transaction history."
            ]

        elif "high value" in q or "amount" in q or "5000" in q:
            recs = [
                "ðŸ’¡ **High-value transaction hold** â€” Add a 10-minute cooling period for first-time high-value transactions (>â‚¹5000) with confirmation SMS to the registered mobile.",
                "ðŸ’¡ **Enhanced KYC for large amounts** â€” Require full KYC verification for users attempting transactions above â‚¹10,000 for the first time.",
                "ðŸ’¡ **AI anomaly detection** â€” Deploy real-time ML models that flag unusual high-value transactions based on user history and peer comparisons."
            ]

        else:
            recs = [
                "ðŸ’¡ **ML fraud scoring** â€” Implement a real-time ML fraud scoring system that assigns risk scores (0-100) to every transaction and auto-blocks scores above 85.",
                "ðŸ’¡ **Device fingerprinting** â€” Add device fingerprinting to detect when a fraudster uses multiple accounts from the same device.",
                "ðŸ’¡ **User fraud reporting** â€” Add a one-tap 'Report Fraud' button in the app so users can flag suspicious transactions instantly, feeding data back to the fraud model."
            ]

    # â”€â”€ Network/performance recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif "network" in q or "success rate" in q:
        recs = [
            "ðŸ’¡ **5G optimisation** â€” Since 5G shows the highest success rate, prioritise partnerships with 5G-enabled payment infrastructure providers.",
            "ðŸ’¡ **3G fallback protocol** â€” For 3G users, use lightweight payment APIs with compressed payloads to reduce data requirements and timeout risks.",
            "ðŸ’¡ **Network quality detection** â€” Measure network quality before transaction initiation and route to lighter API endpoints for poor connections."
        ]

    # â”€â”€ Weekend/time recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif "weekend" in q or "peak hour" in q or "hour" in q:
        recs = [
            "ðŸ’¡ **Peak hour capacity planning** â€” Scale up server capacity during peak transaction hours (identified in data) to handle load spikes without failures.",
            "ðŸ’¡ **Weekend promotions** â€” Since weekend transaction amounts are similar to weekdays, introduce weekend cashback offers to boost volume during lower-traffic periods.",
            "ðŸ’¡ **Off-peak incentives** â€” Offer small rewards for transactions completed during off-peak hours to distribute load more evenly across the day."
        ]

    # â”€â”€ General / merchant recommendations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif "merchant" in q or "categor" in q:
        recs = [
            "ðŸ’¡ **Merchant-specific payment optimisation** â€” Work with high-value merchant categories (Education, Healthcare) to optimise their payment integration for lower failure rates.",
            "ðŸ’¡ **Merchant dashboard** â€” Provide merchants with real-time transaction analytics so they can monitor their own payment success rates.",
            "ðŸ’¡ **Category-based offers** â€” Use high-spending categories (Education, Healthcare) for targeted cashback campaigns to drive more volume."
        ]

    if recs:
        return "\n\n**ðŸ› ï¸ Recommended Actions:**\n" + "\n".join(recs)
    return ""


# â”€â”€ Ambiguity Detector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_ambiguity(query):
    """
    Detects vague/ambiguous queries and returns a helpful clarification response.
    Returns None if query is clear enough to process.
    """
    q = query.lower().strip()

    # Casual greetings â€” respond warmly
    data_keywords = ["fail","fraud","bank","state","age","device","network","merchant","transaction","amount","rate","peak","trend","volume","month","hour","weekend","p2p","p2m","recharge","bill"]
    casual_keywords = ["how are you","how r u","how are u","how am i","how are we","guess how",
                       "whats up","what's up","sup","hey","heyy","good morning","good evening",
                       "good afternoon","good night","howdy","hiya","yo ","hello there","how do you do",
                       "how ya","how is your","how are things"]
    is_casual = any(phrase in q for phrase in casual_keywords)
    has_data   = any(k in q for k in data_keywords)
    if is_casual and not has_data:
        return (
            "ðŸ˜Š I\'m doing great, always ready to crunch 250,000 UPI transactions!\n\n"
            "As for you â€” I\'d guess you\'re curious about your payment data ðŸ˜„\n\n"
            "What would you like to explore today?\n"
            "â€¢ *\'Give me a complete diagnosis of our payment system\'*\n"
            "â€¢ *\'Which bank has the highest failure rate?\'*\n"
            "â€¢ *\'Show me monthly transaction trends\'*"
        )

    # Too short / greeting
    if len(q.split()) <= 2 and not any(k in q for k in ["fraud","fail","bank","state","age","device","network","p2p","p2m","amount","rate","peak","trend","month","day","hour","success","merchant","recharge","avg","amt","txn","pct","count","volume","dig","deeper","more","drill","breakdown","elaborate","compare","failure","fraud","trend","bank","state","device","network","merchant","avg","amt","txn","pct","count","volume"]):
        return (
            "ðŸ‘‹ Hi! I'm InsightX, your UPI payment analytics assistant. "
            "Here are some things you can ask me:\n\n"
            "â€¢ **Failure rates** â€” *'Compare failure rates across all transaction types'*\n"
            "â€¢ **Fraud analysis** â€” *'Which state has the most flagged transactions?'*\n"
            "â€¢ **Trends** â€” *'Show monthly transaction volume for 2024'*\n"
            "â€¢ **Segmentation** â€” *'Which age group spends the most on weekends?'*\n"
            "â€¢ **Risk** â€” *'What % of high-value transactions are flagged?'*\n\n"
            "What would you like to explore?"
        )

    # Vague fraud query
    if q.strip('"' ) in ["fraud", "tell me about fraud", "fraud analysis", "show fraud", "fraud data"] or q == "fraud":
        return (
            "ðŸ” I can analyse fraud from multiple angles. Which would you like?\n\n"
            "1. *'Which state has the most flagged transactions?'*\n"
            "2. *'Which bank has the highest fraud flag rate?'*\n"
            "3. *'What % of transactions above â‚¹5000 are flagged?'*\n"
            "4. *'Which age group has the highest fraud flag rate on weekends?'*\n"
            "5. *'Which network type has the most flagged transactions?'*"
        )

    # Vague "compare banks" â€” which metric?
    if ("compare" in q or "comparison" in q) and "bank" in q and not any(k in q for k in ["fail","success","amount","fraud","flag","volume","count"]):
        return (
            "ðŸ¦ I can compare banks by different metrics. Which one?\n\n"
            "1. *'Compare failure rates across all banks'*\n"
            "2. *'Compare fraud flag rates across all banks'*\n"
            "3. *'Compare average transaction amount by bank'*\n"
            "4. *'Which bank has the most failed transactions?'*"
        )

    # Vague "how are we doing" / "summary" / "overview"
    if any(phrase in q for phrase in ["how are we doing","give me a summary","overview","summary","tell me everything","what do you know","general"]):
        return (
            "ðŸ“Š Here's a quick executive summary approach â€” which area do you want to dive into?\n\n"
            "â€¢ **Operations** â€” failure rates by transaction type, device, network\n"
            "â€¢ **Risk** â€” fraud flagging patterns by state, bank, age group\n"
            "â€¢ **Revenue** â€” average amounts by merchant category, state, age group\n"
            "â€¢ **Trends** â€” monthly volumes, peak hours, weekend vs weekday\n\n"
            "Or just ask *'Show me the most important insight from this data'* for an auto-summary!"
        )

    # "Summarise our conversation / everything we discussed" â†’ LLM summarises history
    if any(phrase in q for phrase in ["summarise everything","summarize everything",
        "summarise what we","summarize what we","discussed so far","we've discussed",
        "weve discussed","so far in this","summarise all","summarize all"]):
        return None  # Pass to LLM with full conversation history

    # Strategic/innovation questions â†’ bypass CEO handler, go to LLM
    if any(phrase in q for phrase in ["which 3 metrics","3 metrics","ceo dashboard and why",
        "what 3 metrics","top 3 metrics","put on a dashboard"]):
        return None  # LLM answers with real strategic reasoning

    # CEO / presentation handler
    if any(phrase in q for phrase in ["ceo","presenting to","board meeting","executive",
        "highlight","key points","bullet points","summarise","summarize","3 points","top 3","what should i"]):
        return (
            "ðŸ‘” Here\'s what I\'d highlight for your CEO presentation:\n\n"
            "**1. Overall Health** â†’ Ask: *\'Give me a complete diagnosis of our payment system\'*\n"
            "**2. Biggest Risk** â†’ Ask: *\'Give me a complete fraud risk profile\'*\n"
            "**3. Top Priority Fix** â†’ Ask: *\'If we had to fix one thing today what would it be?\'*\n\n"
            "Run these 3 queries for a compelling executive presentation! ðŸŽ¯"
        )

    # Vague "something interesting" / "surprise me"
    if any(phrase in q for phrase in ["show me everything","everything","all data","all insights","full report","complete analysis"]):
        return (
            "ðŸ“Š I can\'t show everything at once, but here are the most impactful areas:\n\n"
            "â€¢ *\'Compare failure rates across all transaction types\'*\n"
            "â€¢ *\'Which state has the most flagged transactions?\'*\n"
            "â€¢ *\'Show monthly transaction volume for 2024\'*\n"
            "â€¢ *\'Which age group spends the most on weekends?\'*\n"
            "â€¢ *\'What % of high-value transactions are flagged?\'*\n\n"
            "Which would you like to start with?"
        )

    if any(phrase in q for phrase in ["what should i focus","how to reduce failure","reduce failure","improve success","what to improve","recommendations","suggest","advice"]):
        failed = "result = {\n  \'Highest failure type\': df.groupby(\'transaction_type\')[\'transaction_status\'].apply(lambda x: (x==\'FAILED\').mean()*100).idxmax(),\n}"
        return (
            "ðŸ’¡ Based on the data, here are key areas to focus on:\n\n"
            "â€¢ *\'Compare failure rates across all transaction types\'* â€” find the worst performer\n"
            "â€¢ *\'Which network type has the highest failure rate?\'* â€” infrastructure issues\n"
            "â€¢ *\'Which bank has the most failed transactions?\'* â€” banking partner issues\n"
            "â€¢ *\'What are the peak hours for failed transactions?\'* â€” load/timing issues\n"
            "â€¢ *\'Which state has the highest failure rate?\'* â€” regional issues\n\n"
            "Or ask: *\'Show me the most important insight from this data\'* for a full diagnosis."
        )

    if any(phrase in q for phrase in ["something interesting","surprise me","interesting insight","what\'s interesting","tell me something"]):
        return (
            "ðŸ’¡ Here are some interesting angles to explore:\n\n"
            "â€¢ *'Which hour of the day has the most failed transactions?'*\n"
            "â€¢ *'Compare weekend vs weekday failure rates'*\n"
            "â€¢ *'Which merchant category has the highest failure rate on weekdays?'*\n"
            "â€¢ *'Which combination of bank and transaction type has the highest failure rate?'*"
        )

    # Vague "what's wrong with payments"
    if any(phrase in q for phrase in ["why are","why is"]) and any(k in q for k in ["recharge","p2p","p2m","bill","payment","fail","failing"]):
        return None  # Let the router handle with actual data

    if any(phrase in q for phrase in ["what\'s wrong","whats wrong","problem","issue"]):
        return (
            "ðŸ”§ Let me help you diagnose payment issues. Try these:\n\n"
            "â€¢ *'Compare failure rates across all transaction types'*\n"
            "â€¢ *'Which network type has the highest failure rate?'*\n"
            "â€¢ *'Which bank has the most failed transactions?'*\n"
            "â€¢ *'What are the peak hours for failed transactions?'*"
        )

    # Vague "show me trends"
    if q in ["trends","show trends","show me trends","trend analysis"] or (q == "trend"):
        return (
            "ðŸ“ˆ Which trend are you looking for?\n\n"
            "1. *'Show monthly transaction volume for 2024'*\n"
            "2. *'Which day of the week has the highest transaction volume?'*\n"
            "3. *'What are the peak hours for transactions?'*\n"
            "4. *'Compare weekend vs weekday transaction amounts'*"
        )

    # â”€â”€ Section 3: Explainability questions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # "How did you calculate", "walk me through", "explain", "what does X mean"
    # These get routed to LLM directly â€” no pandas needed
    explain_triggers = [
        "how did you calculate", "how did you determine", "walk me through",
        "explain how", "explain why", "explain the difference",
        "how is the", "how are the", "how confident",
        "what does", "what do", "in simple terms", "step by step",
        "why does", "why do", "limitations of", "what would", "what additional",
        "how would you", "design a", "build a risk", "which 3 metrics",
        "what machine learning", "production-ready", "scale to",
        "if failure rate crosses", "fraud patterns are shifting",
    ]
    if any(trigger in q for trigger in explain_triggers):
        return None  # Let LLM handle with full context â€” no pandas needed

    return None  # Query is clear â€” proceed normally


# â”€â”€ Context-Aware Query Expander â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def expand_query(query, history):
    """
    Expands follow-up queries using conversation context.
    e.g. 'now filter by weekends' â†’ uses previous query topic
    e.g. 'what about iOS?' â†’ adds iOS filter to previous context
    """
    q = query.lower().strip()

    # Get last user query from history for context
    last_topic = ""
    for msg in reversed(history):
        if msg["role"] == "user":
            last_topic = msg["content"].lower()
            break

    # Also check session state for richer context
    import streamlit as _st
    if hasattr(_st, "session_state") and "last_topic" in _st.session_state:
        if _st.session_state["last_topic"]:
            last_topic = _st.session_state["last_topic"].lower()

    # Follow-up patterns
    if q.startswith("now filter by weekend") or q == "filter by weekends" or q == "weekends only":
        if last_topic:
            return last_topic + " for weekend transactions only"
        return "weekend transaction analysis"

    # "same but only for weekends" / "now show me the same but only for weekends"
    if ("same" in q or "now show" in q) and "weekend" in q:
        if last_topic and ("bank" in last_topic or "fail" in last_topic or "ios" in last_topic):
            return "compare failure rates by bank for weekend transactions only"
        elif last_topic:
            return last_topic + " for weekend transactions only"
        return "compare failure rates for weekend transactions"

    if "ios" in q and len(q.split()) < 6:
        # Always show bank failure rates for iOS â€” safest context-aware default
        return "compare failure rates by bank for iOS device type only"

    if q.startswith("what about android"):
        if last_topic:
            return last_topic.replace("ios","").replace("device","") + " for Android device type"
        return "failure rate for Android transactions"

    if q in ["and p2p?","what about p2p?","for p2p","now p2p"]:
        if last_topic:
            return last_topic + " for P2P transactions only"
        return "P2P transaction failure rate"

    if q in ["and p2m?","what about p2m?","for p2m","now p2m"]:
        if last_topic:
            return last_topic + " for P2M transactions only"
        return "P2M transaction failure rate"

    if "sort by" in q or "order by" in q:
        return last_topic + " " + query if last_topic else query

    # "compare that with fraud rates" â†’ use last topic as context
    if "compare" in q and "fraud" in q and ("that" in q or "this" in q or "it" in q):
        if last_topic and "bank" in last_topic:
            return "compare fraud flag rates by bank"
        elif last_topic and "state" in last_topic:
            return "compare fraud flag rates by state"
        elif last_topic and ("ios" in last_topic or "android" in last_topic or "device" in last_topic):
            return "compare fraud flag rates by device type"
        return "compare fraud flag rates by bank"

    # "which one should I focus on" / "what should I prioritise" 
    if any(p in q for p in ["focus on fixing","focus on first","fix first","prioritise","prioritize","should i focus"]):
        if last_topic:
            return "compare failure rates and fraud rates by bank to identify highest priority bank for intervention"
        return "compare failure rates and fraud rates by bank to identify highest priority"

    # "Go back to X" / "what you said about X earlier"
    if any(phrase in q for phrase in ["go back","earlier","what you said","previous","before","you mentioned"]):
        # Search history for mentioned topic
        # Check for specific bank names first
        banks = ["sbi","hdfc","icici","axis","pnb","kotak","indusind","yes bank"]
        for bank in banks:
            if bank in q:
                return f"show failure rate and fraud rate details for {bank.upper()} bank"

        for keyword, expansion in [
            ("fraud", "give me a complete fraud risk profile by age group state and bank"),
            ("failure", "compare failure rates across all transaction types"),
            ("state", "which state has the most flagged transactions"),
            ("bank", "which bank has the highest failure rate"),
            ("device", "compare failure rates by device type"),
            ("merchant", "average transaction amount by merchant category"),
            ("trend", "show monthly transaction volume for 2024"),
            ("weekend", "compare weekend vs weekday failure rates"),
        ]:
            if keyword in q:
                return expansion
        # If no keyword match, use last topic
        return last_topic if last_topic else query

    # "Why do you think that is" - reframe as data investigation of last topic
    if any(phrase in q for phrase in ["why do you think","why is that","why does that happen"]):
        # Return as explain question â€” LLM reasons about context, no pandas needed
        context = last_topic if last_topic else "payment failure patterns"
        return f"explain why: {context}"

    if any(phrase in q for phrase in ["what causes","reason for"]):
        if last_topic:
            return "what factors contribute to " + last_topic
        return "compare failure rates by device type and network type"

    if any(phrase in q for phrase in ["more details","tell me more","elaborate","explain more",
        "dig deeper","go deeper","deep dive","dive deeper","expand on that",
        "more info","give me more","elaborate more","further analysis","analyse more",
        "break it down","break down","zoom in","drill down"]):
        if last_topic:
            # Add device/network/age breakdown context
            if "fail" in last_topic or "failure" in last_topic:
                return "give me a deeper breakdown of failure rates by device type network type and age group for: " + last_topic
            elif "fraud" in last_topic or "flag" in last_topic:
                return "give me a complete fraud risk profile by age group state and bank"
            return "give me a deeper breakdown of: " + last_topic
        return query

    # Standalone shorthand with no other context â€” expand to full question
    if q in ["avg amt", "average amount", "avg amount"]:
        return "what is the average transaction amount in INR"
    if q in ["txn count", "transaction count", "total transactions", "total txns"]:
        return "what is the total count of transactions by transaction type"
    if q in ["pct", "fraud pct", "fraud percentage", "fraud rate"]:
        return "what percentage of transactions have fraud flag equal to 1"
    if q in ["avg amt by merchant", "avg by merchant", "average by merchant category"]:
        return "average transaction amount by merchant category"

    # Auto-fix common typos / shorthand
    query = query.replace("txn","transaction").replace("amt","amount inr").replace("pct","percentage").replace("avg","average")
    query = query.replace("merchant category","merchant_category").replace("transaction type","transaction_type")

    return query

# â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def show_sidebar_stats(df):
    with st.sidebar:
        st.markdown('<div style="font-family:Space Mono,monospace;font-size:1.5rem;color:#e8eaf0;">ðŸ’³ InsightX</div>', unsafe_allow_html=True)
        st.markdown('<div style="color:#6b7280;font-size:0.82rem;margin-bottom:8px;">Leadership Analytics Dashboard</div>', unsafe_allow_html=True)

        # â”€â”€ New Chat at very top â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if st.button("âœ¨ New Chat", use_container_width=True, key="new_chat_top"):
            for key in ["messages","last_topic","last_result","prefill_query"]:
                st.session_state[key] = [] if key == "messages" else ""
            # Delete autosave so it doesn't reload old chat
            delete_session("autosave_current_session")
            st.rerun()

        st.divider()

        # â”€â”€ Auto-load last session on startup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if "messages" in st.session_state and not st.session_state["messages"]:
            auto_data = load_chat("autosave_current_session")
            if auto_data and auto_data.get("messages"):
                st.session_state["messages"] = auto_data["messages"]
                st.rerun()

        # â”€â”€ Chat History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("**ðŸ’¾ Chat History**")
        col_s1, col_s2 = st.columns([3,1])
        with col_s1:
            save_name = st.text_input(
                "Session name",
                placeholder="e.g. fraud analysis",
                label_visibility="collapsed",
                key="save_name_input"
            )
        with col_s2:
            if st.button("ðŸ’¾", key="save_btn", help="Save chat"):
                if save_name.strip() and st.session_state.get("messages"):
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    session_id = f"{timestamp}_{save_name.strip().replace(' ','_')}"
                    save_chat(session_id, st.session_state["messages"])
                    st.success("Saved!")
                elif not st.session_state.get("messages"):
                    st.warning("Nothing to save!")
                else:
                    st.warning("Enter a name!")

        sessions = [s for s in get_all_sessions() if "autosave" not in s["file"]]
        if sessions:
            st.markdown(f"<div style='font-size:0.74rem;color:#5a5f7a;margin-bottom:4px;'>{len(sessions)} saved session(s)</div>", unsafe_allow_html=True)
            for s in sessions[:5]:
                display_name = s['name'].split('_',2)[-1].replace('_',' ') if s['name'].count('_') >= 2 else s['name']
                with st.expander(f"ðŸ“‚ {display_name}", expanded=False):
                    st.markdown(f"<div style='font-size:0.74rem;color:#5a5f7a;'>{s['saved_at']} Â· {s['msg_count']//2} Q&As</div>", unsafe_allow_html=True)
                    col_l, col_d = st.columns(2)
                    with col_l:
                        if st.button("Load", key=f"load_{s['file']}", use_container_width=True):
                            data = load_chat(s['file'].replace('.json',''))
                            if data:
                                st.session_state["messages"] = data["messages"]
                                st.rerun()
                    with col_d:
                        if st.button("Delete", key=f"del_{s['file']}", use_container_width=True):
                            delete_session(s['file'].replace('.json',''))
                            st.rerun()
        else:
            st.markdown("<div style='font-size:0.78rem;color:#5a5f7a;font-style:italic;padding:4px 0;'>No saved sessions yet â€” chats auto-save!</div>", unsafe_allow_html=True)

        st.divider()

        st.markdown("**ðŸ“Š Dataset Overview**")

        total        = len(df)
        success_rate = (df["transaction_status"] == "SUCCESS").mean() * 100 if "transaction_status" in df.columns else 0
        failure_rate = 100 - success_rate
        fraud_rate   = df["fraud_flag"].mean() * 100 if "fraud_flag" in df.columns else 0
        avg_amt      = df["amount_inr"].mean() if "amount_inr" in df.columns else 0

        col1, col2 = st.columns(2)
        with col1:
            st.markdown(f'<div class="stat-card"><div class="stat-number">{total//1000}K</div><div class="stat-label">Transactions</div></div>', unsafe_allow_html=True)
        with col2:
            st.markdown(f'<div class="stat-card"><div class="stat-number">{success_rate:.1f}%</div><div class="stat-label">Success Rate</div></div>', unsafe_allow_html=True)

        st.markdown("<br>", unsafe_allow_html=True)
        col3, col4 = st.columns(2)
        with col3:
            st.markdown(f'<div class="stat-card"><div class="stat-number">{failure_rate:.1f}%</div><div class="stat-label">Failure Rate</div></div>', unsafe_allow_html=True)
        with col4:
            st.markdown(f'<div class="stat-card"><div class="stat-number">{fraud_rate:.2f}%</div><div class="stat-label">Flagged</div></div>', unsafe_allow_html=True)

        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown(f'<div class="stat-card"><div class="stat-number">â‚¹{avg_amt:.0f}</div><div class="stat-label">Avg Transaction</div></div>', unsafe_allow_html=True)

        st.divider()
        st.markdown("**ðŸ’¡ Sample Questions**")
        suggestions = [
            "Which transaction type has the highest failure rate?",
            "Which age group spends the most on weekends?",
            "Which state has the most flagged transactions?",
            "How does network type affect success rate?",
            "What are peak hours for P2M transactions?",
            "Compare failure rates between Android and iOS",
            "Which merchant category has highest avg amount?",
            "Which bank has most failed transactions?",
        ]
        for s in suggestions:
            if st.button(s, key=f"btn_{s[:20]}", use_container_width=True):
                st.session_state["prefill_query"] = s

        st.divider()

        with st.expander("ðŸ› ï¸ Detected Column Names"):
            st.code(", ".join(df.columns.tolist()))



# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    try:
        df = load_data()
    except FileNotFoundError:
        st.error("âŒ `upi_transactions_2024.csv` not found. Place it in the same folder as app.py.")
        st.stop()

    client     = get_client()
    df_summary = get_data_summary(df)

    if "messages" not in st.session_state:
        st.session_state["messages"] = []
        # Auto-load last session on first run
        auto_data = load_chat("autosave_current_session")
        if auto_data and auto_data.get("messages"):
            st.session_state["messages"] = auto_data["messages"]
    if "prefill_query" not in st.session_state:
        st.session_state["prefill_query"] = ""

    show_sidebar_stats(df)

    st.markdown('<h1 style="font-family:Space Mono,monospace;font-size:2rem;margin-bottom:0;">InsightX ðŸ”</h1>', unsafe_allow_html=True)
    st.markdown('<p style="color:#6b7280;margin-top:4px;">Ask any business question about your UPI transaction data</p>', unsafe_allow_html=True)
    st.divider()

    if not st.session_state["messages"]:
        st.markdown("""
        <div style="text-align:center;padding:60px 20px;color:#4b5563;">
            <div style="font-size:3rem;">ðŸ’¬</div>
            <div style="font-size:1.1rem;margin-top:12px;">Start by asking a question about your payment data</div>
            <div style="font-size:0.85rem;margin-top:8px;">Try the suggestions in the sidebar â†’</div>
        </div>
        """, unsafe_allow_html=True)

    for msg in st.session_state["messages"]:
        if msg["role"] == "user":
            st.markdown(f'<div class="user-msg">ðŸ§‘â€ðŸ’¼ <strong>You:</strong> {msg["content"]}</div>', unsafe_allow_html=True)
        else:
            st.markdown(f'<div class="assistant-msg">ðŸ¤– <strong>InsightX:</strong><br><br>{msg["content"]}</div>', unsafe_allow_html=True)

    prefill    = st.session_state.pop("prefill_query", "")
    user_input = st.chat_input("Ask a question about your transaction data...")
    if prefill and not user_input:
        user_input = prefill

    if user_input:
        st.session_state["messages"].append({"role": "user", "content": user_input})

        # Store context for follow-up queries
        if "last_topic" not in st.session_state:
            st.session_state["last_topic"] = ""
        if "last_result" not in st.session_state:
            st.session_state["last_result"] = ""

        # Stop button
        stop_placeholder = st.empty()
        stop_placeholder.markdown(
            '<div class="stop-btn-container">'
            '<button class="stop-btn" onclick="window.stop_requested=true">â¹ Stop generating</button>'
            '</div>',
            unsafe_allow_html=True
        )
        with st.spinner("ðŸ” Crunching 250,000 transactions..."):
            try:
                # â”€â”€ Step 0: Intent detection & ambiguity handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                clarification = detect_ambiguity(user_input)
                if clarification:
                    st.session_state["messages"].append({"role": "assistant", "content": clarification})
                    st.rerun()

                # â”€â”€ Step 1: Context-aware query expansion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                expanded_query = expand_query(user_input, st.session_state["messages"][:-1])

                # â”€â”€ Step 2: Multi-part question handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # Always check ORIGINAL input for no-split phrases, not expanded
                parts = split_multipart(user_input) if split_multipart(user_input) == [user_input] else split_multipart(expanded_query)
                # If original was a follow-up, never split
                no_split_check = ["tell me more","more about","what about","same but","now show",
                    "why do you","why is that","go back","summarise","summarize","which one should",
                    "focus on","interesting","elaborate","dig deeper","compare that","can you compare"]
                if any(p in user_input.lower() for p in no_split_check):
                    parts = [expanded_query]
                if len(parts) > 1:
                    combined_results = []
                    combined_codes = []
                    for i, part in enumerate(parts):
                        routed = route_query(part, df)
                        c = routed if routed else generate_pandas_code(part, df_summary, st.session_state["messages"][:-1], client)
                        r, e = execute_code(c, df)
                        label = f"QUESTION {i+1}: {part}"
                        result_text = r if r else f"Could not compute: {str(e)}"
                        combined_results.append(f"{label}\nANSWER {i+1}: {result_text}")
                        combined_codes.append(f"# Q{i+1}: {part}\n{c}")
                    data_result = "\n\n---\n\n".join(combined_results)
                    error = None
                    code = "\n\n".join(combined_codes)
                    # Generate one insight per question and combine
                    individual_insights = []
                    for i, (part, res) in enumerate(zip(parts, combined_results)):
                        part_result = res.split(f"ANSWER {i+1}: ")[-1] if f"ANSWER {i+1}:" in res else res
                        part_insight = generate_insight(part, part_result, None, [], client)
                        individual_insights.append(f"**Q{i+1}: {part}**\n{part_insight}")
                    combined_insight = "\n\n---\n\n".join(individual_insights)
                    recs = get_recommendations(" ".join(parts), data_result)
                    if recs:
                        combined_insight += "\n\n" + recs
                    st.session_state["messages"].append({"role": "assistant", "content": combined_insight})
                    st.session_state["last_topic"] = user_input
                    st.session_state["last_result"] = data_result
                else:
                    # Try smart router first, fall back to LLM if no match
                    routed_code = route_query(expanded_query, df)
                    if routed_code:
                        code = routed_code
                        data_result, error = execute_code(code, df)
                    elif expanded_query.lower().startswith("explain why:") or expanded_query.lower().startswith("explain why "):
                        # Pure reasoning question â€” no pandas needed, LLM explains directly
                        data_result, error, code = None, None, "# No code â€” LLM reasoning question"
                    else:
                        code = generate_pandas_code(expanded_query, df_summary, st.session_state["messages"][:-1], client)
                        data_result, error = execute_code(code, df)

                if not st.session_state["messages"] or st.session_state["messages"][-1]["role"] != "assistant":
                    insight = generate_insight(user_input, data_result, error, st.session_state["messages"][:-1], client)
                    recommendations = get_recommendations(user_input, data_result or "")
                    if recommendations:
                        insight = insight + "\n\n" + recommendations
                    st.session_state["messages"].append({"role": "assistant", "content": insight})
                    st.session_state["last_topic"] = expanded_query  # store expanded for better context
                    st.session_state["last_result"] = data_result or ""
                # Auto-save after every message
                save_chat("autosave_current_session", st.session_state["messages"])

                # â”€â”€ Verification Panel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                with st.expander("ðŸ“Š How did InsightX compute this? Click to verify â†’", expanded=False):
                    tab1, tab2 = st.tabs(["ðŸ“‹ Raw Result Table", "ðŸ§‘â€ðŸ’» Generated Code"])

                    with tab1:
                        if error:
                            st.error(f"âŒ Execution error: {error}")
                            st.info("The insight above was generated from context, not live data.")
                        else:
                            st.markdown("**ðŸ“Š Exact data used to generate this answer:**")
                            try:
                                local_vars2 = {"df": df.copy(), "pd": pd}
                                exec(code, {}, local_vars2)
                                raw = local_vars2.get("result")
                                if isinstance(raw, pd.DataFrame):
                                    st.dataframe(raw, use_container_width=True)
                                elif isinstance(raw, dict):
                                    # Flatten nested dicts for display
                                    flat_items = []
                                    for k, v in raw.items():
                                        if isinstance(v, dict):
                                            for k2, v2 in v.items():
                                                flat_items.append({
                                                    "Category": str(k),
                                                    "Sub-category": str(k2),
                                                    "Value": round(v2, 4) if isinstance(v2, float) else v2
                                                })
                                        else:
                                            flat_items.append({
                                                "Category": str(k),
                                                "Sub-category": "â€”",
                                                "Value": round(v, 4) if isinstance(v, float) else v
                                            })
                                    if flat_items:
                                        st.dataframe(pd.DataFrame(flat_items), use_container_width=True)
                                else:
                                    st.code(str(raw))
                            except Exception:
                                st.text(data_result[:3000] if data_result else "No result")

                            st.markdown("---")
                            col_a, col_b, col_c = st.columns(3)
                            with col_a:
                                st.metric("ðŸ“¦ Total Rows Analysed", f"{len(df):,}")
                            with col_b:
                                success_pct = round((df['transaction_status']=='SUCCESS').mean()*100, 1)
                                st.metric("âœ… Overall Success Rate", f"{success_pct}%")
                            with col_c:
                                st.metric("ðŸ“… Data Period", "Janâ€“Dec 2024")
                            st.success("âœ… Numbers computed directly from 250,000 row CSV using Pandas â€” zero AI hallucination.")

                    with tab2:
                        st.markdown("**LLM-generated Pandas code (executed live on your data):**")
                        st.code(code, language="python")
                        st.info("ðŸ’¡ Copy this code into a Jupyter notebook or Python script to independently verify the answer.")

            except Exception as e:
                err_str = str(e)
                if "connection" in err_str.lower() or "refused" in err_str.lower():
                    msg = ("âš ï¸ **Connection issue** â€” I can't reach the AI model right now.\n\n"
                           "Please make sure **LM Studio** is running at `http://127.0.0.1:1234` with a model loaded, then try again.")
                else:
                    msg = (f"âš ï¸ **Something went wrong** â€” I hit an unexpected error.\n\n"
                           f"Try rephrasing your question, or pick one from the sidebar suggestions.\n\n"
                           f"*Technical detail: {err_str[:100]}*")
                st.session_state["messages"].append({"role": "assistant", "content": msg})

        stop_placeholder.empty()
        st.rerun()

if __name__ == "__main__":
    main()